{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Themis Documentation</p> Evaluate LLMs with confidence. <p>     Themis gives research teams a clean workflow for running benchmarks,     tracking experiments, and comparing models with statistical rigor.   </p> Start Quickstart View CLI Guide"},{"location":"#choose-your-path","title":"Choose Your Path","text":"Install <p>Set up Themis and verify your environment.</p> Run First Eval <p>Use `evaluate(...)` and get your first metrics quickly.</p> Design Experiments <p>Move from one-liners to spec/session-driven workflows.</p> Compare Runs <p>Analyze run deltas with statistical tests.</p> Explore Benchmarks <p>Browse built-in benchmarks and expected formats.</p> Dive Into API <p>Use the full Python API and extension points.</p>"},{"location":"#core-workflow","title":"Core Workflow","text":"<pre><code>flowchart LR\n    A[\"Benchmark / Dataset\"] --&gt; B[\"themis.evaluate(...) or ExperimentSession.run(...)\"]\n    B --&gt; C[\"Generation + Evaluation\"]\n    C --&gt; D[\"ExperimentStorage\"]\n    D --&gt; E[\"compare_runs(...) / Export / API Server\"]</code></pre>"},{"location":"#quick-recipes","title":"Quick Recipes","text":"EvaluateCompareServe Dashboard <pre><code>themis eval gsm8k --model gpt-4 --limit 100 --run-id gsm8k-gpt4\n</code></pre> <pre><code>themis compare gsm8k-gpt4 gsm8k-claude --output comparison.html\n</code></pre> <pre><code>themis serve --storage .cache/experiments\n</code></pre>"},{"location":"#documentation-map","title":"Documentation Map","text":"<ul> <li>Getting Started: installation, quickstart, and core concepts.</li> <li>Guides: evaluation design, storage, providers, comparison, and interoperability.</li> <li>Reference: session, specs, benchmarks, and API server details.</li> <li>Python API: function/class-level API behavior.</li> <li>Architecture: module boundaries and design decisions.</li> </ul>"},{"location":"#runnable-examples","title":"Runnable Examples","text":"<ul> <li>examples-simple/01_quickstart.py</li> <li>examples-simple/02_custom_dataset.py</li> <li>examples-simple/04_comparison.py</li> <li>examples-simple/08_resume_cache.py</li> <li>examples-simple/09_research_loop.py</li> </ul>"},{"location":"ARCHITECTURE/","title":"Architecture Overview","text":"<p>Themis has two public evaluation surfaces built on the same orchestration core:</p> <ul> <li><code>themis.evaluate(...)</code>: high-level API for benchmark and dataset workflows.</li> <li><code>ExperimentSession().run(spec, ...)</code>: explicit spec/session API for advanced control.</li> </ul>"},{"location":"ARCHITECTURE/#layered-design","title":"Layered Design","text":"<pre><code>themis.evaluate(...) / CLI commands\n            |\n       ExperimentSession\n            |\n   ExperimentSpec / ExecutionSpec / StorageSpec\n            |\nGenerationPlan -&gt; GenerationRunner -&gt; EvaluationPipelineContract\n            |\n  ExperimentStorage / comparison / server / export\n</code></pre>"},{"location":"ARCHITECTURE/#primary-components","title":"Primary Components","text":""},{"location":"ARCHITECTURE/#public-api-layer","title":"Public API Layer","text":"<ul> <li><code>themis.api.evaluate</code>: convenience wrapper that resolves presets, metrics, and defaults.</li> <li><code>themis.session.ExperimentSession</code>: explicit orchestrator entrypoint.</li> <li><code>themis.cli.main</code>: <code>demo</code>, <code>eval</code>, <code>compare</code>, <code>share</code>, <code>serve</code>, <code>list</code>, <code>clean</code>.</li> </ul>"},{"location":"ARCHITECTURE/#spec-layer","title":"Spec Layer","text":"<ul> <li><code>themis.specs.ExperimentSpec</code>: dataset, prompt, model, sampling, pipeline, run id.</li> <li><code>themis.specs.ExecutionSpec</code>: worker/retry policy and optional execution backend.</li> <li><code>themis.specs.StorageSpec</code>: storage path/backend and caching toggle.</li> </ul>"},{"location":"ARCHITECTURE/#generation-evaluation-layer","title":"Generation + Evaluation Layer","text":"<ul> <li><code>themis.generation.GenerationPlan</code>: expands dataset into tasks.</li> <li><code>themis.generation.GenerationRunner</code>: executes tasks against providers.</li> <li><code>themis.evaluation.EvaluationPipelineContract</code>: enforced evaluation interface.</li> <li><code>themis.evaluation.EvaluationPipeline</code> / <code>MetricPipeline</code>: standard metric execution.</li> </ul>"},{"location":"ARCHITECTURE/#persistence-analysis-layer","title":"Persistence + Analysis Layer","text":"<ul> <li><code>themis.storage.ExperimentStorage</code>: filesystem-backed run storage.</li> <li><code>themis.comparison.compare_runs</code>: statistical run-to-run comparison.</li> <li><code>themis.experiment.export</code>: CSV/JSON/HTML export utilities.</li> <li><code>themis.server.create_app</code>: REST/WebSocket API over run artifacts.</li> </ul>"},{"location":"ARCHITECTURE/#data-contracts","title":"Data Contracts","text":"<ul> <li>Generation outputs are represented by <code>GenerationRecord</code>.</li> <li>Evaluation outputs are represented by <code>EvaluationRecord</code> where <code>scores</code> is <code>list[MetricScore]</code>.</li> <li>Run-level output is <code>ExperimentReport</code>.</li> </ul> <p>This canonical shape is used consistently by storage, comparison, exports, and server endpoints.</p>"},{"location":"ARCHITECTURE/#model-routing-contract","title":"Model Routing Contract","text":"<ul> <li>Recommended model key format: <code>provider:model_id</code> (for example <code>litellm:gpt-4</code>).</li> <li>High-level <code>evaluate(...)</code> also accepts provider-auto-detected model strings (for example <code>gpt-4</code>).</li> </ul>"},{"location":"ARCHITECTURE/#extension-points","title":"Extension Points","text":"<ul> <li>Metrics: <code>themis.register_metric(name, metric_cls)</code></li> <li>Datasets: <code>themis.register_dataset(name, factory)</code></li> <li>Providers: <code>themis.register_provider(name, factory)</code></li> <li>Benchmarks: <code>themis.register_benchmark(preset)</code></li> <li>Backends: custom <code>ExecutionBackend</code> and <code>StorageBackend</code></li> </ul>"},{"location":"ARCHITECTURE/#current-trade-offs","title":"Current Trade-offs","text":"<ul> <li><code>distributed=True</code> in <code>evaluate(...)</code>/CLI is reserved and currently not wired.</li> <li>Custom dataset files are not yet supported in <code>themis eval ...</code> CLI; use Python API for dataset objects.</li> <li>Custom storage backends must be ExperimentStorage-compatible for full session integration.</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>Canonical release history lives in the repository root at <code>CHANGELOG.md</code>.</p>"},{"location":"CHANGELOG/#current-release","title":"Current Release","text":"<ul> <li>Version: <code>1.0.0</code></li> <li>Date: <code>2026-02-04</code></li> <li>Highlights:</li> <li>Promoted spec/session architecture to stable <code>1.0.0</code>.</li> <li>Removed legacy builder module and related tests.</li> <li>Standardized experiment exports and module surface.</li> <li>Fixed <code>CacheManager.get_run_path()</code> so <code>None</code> paths do not create <code>None/report.json</code> artifacts.</li> </ul> <p>For full historical notes and migration details, use the root changelog.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Themis","text":"<p>Thank you for your interest in contributing to Themis! We welcome contributions from the community.</p> <p>This guide covers: - Development setup - Code style and standards - Testing requirements - Pull request process - Areas where we need help</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":""},{"location":"CONTRIBUTING/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>uv (recommended for dependency management)</li> </ul>"},{"location":"CONTRIBUTING/#installation","title":"Installation","text":"<ol> <li>Fork the repository on GitHub.</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/yourusername/themis.git\ncd themis\n</code></pre></li> <li>Install dependencies using <code>uv</code>:    <pre><code>uv sync\n</code></pre>    This will create a virtual environment and install all necessary dependencies, including development tools.</li> </ol>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":""},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<p>We use <code>pytest</code> for testing. To run the full test suite:</p> <pre><code>uv run pytest\n</code></pre> <p>To run a specific test file:</p> <pre><code>uv run pytest tests/generation/test_strategies.py\n</code></pre> <p>To run with coverage:</p> <pre><code>uv run pytest --cov=themis --cov-report=html\n</code></pre>"},{"location":"CONTRIBUTING/#code-style","title":"Code Style","text":"<ul> <li>Python Version: 3.12+</li> <li>Formatting: We follow PEP 8.</li> <li>Type Hinting: All code should be fully type-hinted and pass static analysis.</li> <li>Docstrings: Please include docstrings for all public modules, classes, and functions.</li> </ul>"},{"location":"CONTRIBUTING/#project-structure","title":"Project Structure","text":"<ul> <li><code>themis/</code>: Source code</li> <li><code>tests/</code>: Test suite</li> <li><code>examples-simple/</code>: Runnable example scripts</li> <li><code>docs/</code>: Documentation</li> </ul>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Create a new branch for your feature or bugfix:    <pre><code>git checkout -b feature/my-new-feature\n</code></pre></li> <li>Make your changes and commit them with clear, descriptive messages.</li> <li>Run tests to ensure your changes don't break existing functionality.</li> <li>Push your branch to your fork:    <pre><code>git push origin feature/my-new-feature\n</code></pre></li> <li>Open a Pull Request against the <code>main</code> branch of the original repository.</li> <li>Provide a clear title and description of your changes.</li> <li>Link to any relevant issues.</li> </ol>"},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting Issues","text":"<p>If you find a bug or have a feature request, please open an issue on GitHub. Provide as much detail as possible, including steps to reproduce the issue.</p>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"api/backends/","title":"Backends API","text":"<p>Themis supports pluggable execution and storage backends.</p>"},{"location":"api/backends/#storage","title":"Storage","text":"<ul> <li><code>themis.backends.storage.StorageBackend</code></li> <li><code>themis.backends.storage.LocalFileStorageBackend</code></li> <li><code>themis.storage.ExperimentStorage</code></li> </ul> <p>Example with <code>evaluate()</code>:</p> <pre><code>from themis import evaluate\nfrom themis.backends.storage import LocalFileStorageBackend\n\nreport = evaluate(\n    \"demo\",\n    model=\"fake-math-llm\",\n    storage_backend=LocalFileStorageBackend(\".cache/experiments\"),\n)\n</code></pre>"},{"location":"api/backends/#execution","title":"Execution","text":"<ul> <li><code>themis.backends.execution.ExecutionBackend</code></li> <li><code>themis.backends.execution.LocalExecutionBackend</code></li> <li><code>themis.backends.execution.SequentialExecutionBackend</code></li> </ul> <p>Example with specs:</p> <pre><code>from themis.backends.execution import LocalExecutionBackend\nfrom themis.session import ExperimentSession\nfrom themis.specs import ExecutionSpec\n\nreport = ExperimentSession().run(\n    spec,\n    execution=ExecutionSpec(backend=LocalExecutionBackend(max_workers=8), workers=8),\n)\n</code></pre>"},{"location":"api/backends/#notes","title":"Notes","text":"<ul> <li><code>evaluate()</code> and <code>ExperimentSession</code> currently require storage backends that are   ExperimentStorage-compatible.</li> <li>Custom backend interfaces are stable, but full custom storage integration into   the high-level API is still evolving.</li> </ul>"},{"location":"api/comparison/","title":"Comparison API","text":"<p>Statistical comparison of experiment runs.</p>"},{"location":"api/comparison/#functions","title":"Functions","text":""},{"location":"api/comparison/#compare_runs","title":"compare_runs()","text":"<p>Compare multiple experiment runs with statistical significance testing.</p> <pre><code>def compare_runs(\n    run_ids: Sequence[str],\n    *,\n    storage_path: str | Path,\n    metrics: Sequence[str] | None = None,\n    statistical_test: StatisticalTest = StatisticalTest.BOOTSTRAP,\n    alpha: float = 0.05,\n) -&gt; ComparisonReport:\n</code></pre>"},{"location":"api/comparison/#parameters","title":"Parameters","text":"<ul> <li><code>run_ids</code> : <code>Sequence[str]</code> - List of run IDs to compare (minimum 2)</li> <li><code>storage_path</code> : <code>str | Path</code> - Path to experiment storage</li> <li><code>metrics</code> : <code>Sequence[str] | None</code> - Metrics to compare (None = all)</li> <li><code>statistical_test</code> : <code>StatisticalTest</code> - Type of statistical test</li> <li><code>alpha</code> : <code>float</code> - Significance level (default: 0.05 for 95% confidence)</li> </ul>"},{"location":"api/comparison/#returns","title":"Returns","text":"<p><code>ComparisonReport</code> - Contains pairwise results, win/loss matrices, and rankings</p>"},{"location":"api/comparison/#example","title":"Example","text":"<pre><code>from themis.comparison import compare_runs\nfrom themis.comparison.statistics import StatisticalTest\n\nreport = compare_runs(\n    run_ids=[\"gpt4-run\", \"claude-run\"],\n    storage_path=\".cache/experiments\",\n    statistical_test=StatisticalTest.BOOTSTRAP,\n    alpha=0.05,\n)\n\nprint(report.summary())\n</code></pre>"},{"location":"api/comparison/#classes","title":"Classes","text":""},{"location":"api/comparison/#comparisonreport","title":"ComparisonReport","text":"<p>Comprehensive comparison report for multiple runs.</p>"},{"location":"api/comparison/#attributes","title":"Attributes","text":"<ul> <li><code>run_ids</code> : <code>list[str]</code> - All run IDs being compared</li> <li><code>metrics</code> : <code>list[str]</code> - Metrics being compared</li> <li><code>pairwise_results</code> : <code>list[ComparisonResult]</code> - All pairwise comparisons</li> <li><code>win_loss_matrices</code> : <code>dict[str, WinLossMatrix]</code> - Matrix for each metric</li> <li><code>best_run_per_metric</code> : <code>dict[str, str]</code> - Best run for each metric</li> <li><code>overall_best_run</code> : <code>str | None</code> - Overall best run</li> </ul>"},{"location":"api/comparison/#methods","title":"Methods","text":"<p><code>get_comparison(run_a: str, run_b: str, metric: str) -&gt; ComparisonResult | None</code></p> <p>Get specific pairwise comparison.</p> <p><code>get_metric_results(metric: str) -&gt; list[ComparisonResult]</code></p> <p>Get all comparisons for a metric.</p> <p><code>summary(include_details: bool = False) -&gt; str</code></p> <p>Generate human-readable summary.</p> <p><code>to_dict() -&gt; dict</code></p> <p>Convert to dictionary for serialization.</p>"},{"location":"api/comparison/#example_1","title":"Example","text":"<pre><code>report = compare_runs([\"run-1\", \"run-2\"], storage_path=\".cache\")\n\n# Access results\nbest = report.overall_best_run\nprint(f\"Winner: {best}\")\n\n# Get specific comparison\nresult = report.get_comparison(\"run-1\", \"run-2\", \"ExactMatch\")\nif result and result.is_significant():\n    print(f\"Significant difference: {result.delta:.3f}\")\n</code></pre>"},{"location":"api/comparison/#comparisonresult","title":"ComparisonResult","text":"<p>Result of comparing two runs on a single metric.</p>"},{"location":"api/comparison/#attributes_1","title":"Attributes","text":"<ul> <li><code>metric_name</code> : <code>str</code> - Metric being compared</li> <li><code>run_a_id</code> : <code>str</code> - First run identifier</li> <li><code>run_b_id</code> : <code>str</code> - Second run identifier</li> <li><code>run_a_mean</code> : <code>float</code> - Mean score for run A</li> <li><code>run_b_mean</code> : <code>float</code> - Mean score for run B</li> <li><code>delta</code> : <code>float</code> - Difference (run_a - run_b)</li> <li><code>delta_percent</code> : <code>float</code> - Percentage difference</li> <li><code>winner</code> : <code>str</code> - Winner ID or \"tie\"</li> <li><code>test_result</code> : <code>TestResult | None</code> - Statistical test result</li> </ul>"},{"location":"api/comparison/#methods_1","title":"Methods","text":"<p><code>is_significant() -&gt; bool</code></p> <p>Check if difference is statistically significant.</p> <p><code>summary() -&gt; str</code></p> <p>Generate human-readable summary.</p>"},{"location":"api/comparison/#winlossmatrix","title":"WinLossMatrix","text":"<p>Win/loss/tie matrix for comparing multiple runs.</p>"},{"location":"api/comparison/#attributes_2","title":"Attributes","text":"<ul> <li><code>run_ids</code> : <code>list[str]</code> - Run IDs in the matrix</li> <li><code>metric_name</code> : <code>str</code> - Metric being compared</li> <li><code>matrix</code> : <code>list[list[str]]</code> - 2D matrix of results</li> <li><code>win_counts</code> : <code>dict[str, int]</code> - Wins for each run</li> <li><code>loss_counts</code> : <code>dict[str, int]</code> - Losses for each run</li> <li><code>tie_counts</code> : <code>dict[str, int]</code> - Ties for each run</li> </ul>"},{"location":"api/comparison/#methods_2","title":"Methods","text":"<p><code>get_result(run_a: str, run_b: str) -&gt; str</code></p> <p>Get comparison result between two runs.</p> <p><code>rank_runs() -&gt; list[tuple[str, int, int, int]]</code></p> <p>Rank runs by performance (wins, then losses).</p> <p><code>to_table() -&gt; str</code></p> <p>Generate formatted table representation.</p>"},{"location":"api/comparison/#example_2","title":"Example","text":"<pre><code>report = compare_runs(\n    [\"run-1\", \"run-2\", \"run-3\"],\n    storage_path=\".cache\",\n)\n\nmatrix = report.win_loss_matrices[\"ExactMatch\"]\n\n# View table\nprint(matrix.to_table())\n\n# Get rankings\nfor run_id, wins, losses, ties in matrix.rank_runs():\n    print(f\"{run_id}: {wins}W-{losses}L-{ties}T\")\n</code></pre>"},{"location":"api/comparison/#statistical-tests","title":"Statistical Tests","text":""},{"location":"api/comparison/#statisticaltest","title":"StatisticalTest","text":"<p>Enum of available statistical tests.</p> <pre><code>from themis.comparison.statistics import StatisticalTest\n\n# Available tests\nStatisticalTest.T_TEST        # Student's t-test\nStatisticalTest.BOOTSTRAP     # Bootstrap confidence intervals\nStatisticalTest.PERMUTATION   # Permutation test\nStatisticalTest.NONE          # No testing\n</code></pre>"},{"location":"api/comparison/#t_test","title":"t_test()","text":"<p>Perform Student's t-test.</p> <pre><code>from themis.comparison.statistics import t_test\n\nresult = t_test(\n    samples_a=[0.8, 0.85, 0.82],\n    samples_b=[0.7, 0.75, 0.72],\n    alpha=0.05,\n    paired=True,\n)\n\nprint(f\"p-value: {result.p_value}\")\nprint(f\"Significant: {result.significant}\")\nprint(f\"Effect size: {result.effect_size}\")\n</code></pre>"},{"location":"api/comparison/#bootstrap_confidence_interval","title":"bootstrap_confidence_interval()","text":"<p>Compute bootstrap confidence interval.</p> <pre><code>from themis.comparison.statistics import bootstrap_confidence_interval\n\nresult = bootstrap_confidence_interval(\n    samples_a=[0.8, 0.85, 0.82],\n    samples_b=[0.7, 0.75, 0.72],\n    n_bootstrap=10000,\n    confidence_level=0.95,\n    seed=42,\n)\n\nprint(f\"CI: {result.confidence_interval}\")\nprint(f\"Significant: {result.significant}\")\n</code></pre>"},{"location":"api/comparison/#permutation_test","title":"permutation_test()","text":"<p>Perform permutation test.</p> <pre><code>from themis.comparison.statistics import permutation_test\n\nresult = permutation_test(\n    samples_a=[0.8, 0.85, 0.82],\n    samples_b=[0.7, 0.75, 0.72],\n    n_permutations=10000,\n    alpha=0.05,\n    seed=42,\n)\n\nprint(f\"p-value: {result.p_value}\")\n</code></pre>"},{"location":"api/comparison/#complete-example","title":"Complete Example","text":"<pre><code>from themis import evaluate\nfrom themis.comparison import compare_runs\nfrom themis.comparison.statistics import StatisticalTest\n\n# Run two experiments\nresult1 = evaluate(\"gsm8k\",\n    model=\"gpt-4\",\n    temperature=0.0,\n    run_id=\"gpt4-temp0\",\n    limit=100,\n)\n\nresult2 = evaluate(\"gsm8k\",\n    model=\"gpt-4\",\n    temperature=0.7,\n    run_id=\"gpt4-temp07\",\n    limit=100,\n)\n\n# Compare with bootstrap test\nreport = compare_runs(\n    run_ids=[\"gpt4-temp0\", \"gpt4-temp07\"],\n    storage_path=\".cache/experiments\",\n    statistical_test=StatisticalTest.BOOTSTRAP,\n    alpha=0.05,\n)\n\n# View results\nprint(report.summary(include_details=True))\n\n# Check significance\nfor result in report.pairwise_results:\n    if result.is_significant():\n        print(f\"{result.metric_name}: {result.winner} wins (p={result.test_result.p_value:.4f})\")\n    else:\n        print(f\"{result.metric_name}: no significant difference\")\n\n# Export\nimport json\nPath(\"comparison.json\").write_text(json.dumps(report.to_dict(), indent=2))\n</code></pre>"},{"location":"api/comparison/#see-also","title":"See Also","text":"<ul> <li>Comparison Guide - Detailed usage guide</li> <li>CLI Reference - Command-line usage</li> </ul>"},{"location":"api/evaluate/","title":"evaluate() API","text":"<p>Primary entry point for running LLM evaluations.</p>"},{"location":"api/evaluate/#signature","title":"Signature","text":"<pre><code>def evaluate(\n    benchmark_or_dataset: str | Sequence[dict[str, Any]],\n    *,\n    model: str,\n    limit: int | None = None,\n    prompt: str | None = None,\n    metrics: list[str] | None = None,\n    temperature: float = 0.0,\n    max_tokens: int = 512,\n    num_samples: int = 1,\n    distributed: bool = False,\n    workers: int = 4,\n    storage: str | Path | None = None,\n    storage_backend: object | None = None,\n    execution_backend: object | None = None,\n    run_id: str | None = None,\n    resume: bool = True,\n    on_result: Callable[[GenerationRecord], None] | None = None,\n    **kwargs: Any,\n) -&gt; ExperimentReport:\n</code></pre>"},{"location":"api/evaluate/#parameters","title":"Parameters","text":""},{"location":"api/evaluate/#required","title":"Required","text":"<p><code>benchmark_or_dataset</code> : <code>str | Sequence[dict[str, Any]]</code></p> <ul> <li>Benchmark name (e.g., <code>\"gsm8k\"</code>, <code>\"math500\"</code>)</li> <li>Or custom dataset as a list of dictionaries</li> </ul> <p>For custom datasets, each dict should include: - <code>prompt</code> / <code>question</code> (input) - <code>answer</code> / <code>reference</code> (expected output) - Optional <code>id</code> / <code>unique_id</code></p> <p><code>model</code> : <code>str</code></p> <p>Model identifier for provider routing. Examples: - <code>\"gpt-4\"</code> - <code>\"claude-3-opus-20240229\"</code> - <code>\"azure/gpt-4\"</code> - <code>\"ollama/llama3\"</code></p>"},{"location":"api/evaluate/#optional","title":"Optional","text":"<p><code>limit</code> : <code>int | None = None</code></p> <p>Maximum number of samples to evaluate.</p> <p><code>prompt</code> : <code>str | None = None</code></p> <p>Custom prompt template using Python format fields (e.g. <code>\"Q: {question}\\nA:\"</code>). If <code>None</code>, uses the benchmark preset template.</p> <p><code>metrics</code> : <code>list[str] | None = None</code></p> <p>Metric names to compute. If <code>None</code>, uses preset defaults. Example names: - <code>\"exact_match\"</code> - <code>\"math_verify\"</code> - <code>\"bleu\"</code>, <code>\"rouge1\"</code>, <code>\"bertscore\"</code>, <code>\"meteor\"</code> - <code>\"pass_at_k\"</code>, <code>\"execution_accuracy\"</code>, <code>\"codebleu\"</code></p> <p>Metric names are normalized (case-insensitive; <code>ExactMatch</code> and <code>exact_match</code> both work).</p> <p><code>temperature</code> : <code>float = 0.0</code></p> <p>Sampling temperature.</p> <p><code>max_tokens</code> : <code>int = 512</code></p> <p>Maximum tokens generated per response.</p> <p><code>num_samples</code> : <code>int = 1</code></p> <p>Number of samples per prompt. This is currently only partially wired in the spec/session flow.</p> <p><code>distributed</code> : <code>bool = False</code></p> <p>Reserved for future distributed execution. Currently ignored.</p> <p><code>workers</code> : <code>int = 4</code></p> <p>Parallel worker count for generation.</p> <p><code>storage</code> : <code>str | Path | None = None</code></p> <p>Storage location for runs and cache. Defaults to <code>.cache/experiments</code>.</p> <p><code>storage_backend</code> : <code>object | None = None</code></p> <p>Optional storage backend instance (typically <code>ExperimentStorage</code> or <code>LocalFileStorageBackend</code>). Custom storage backends are not yet wired into <code>ExperimentSession</code>.</p> <p><code>execution_backend</code> : <code>object | None = None</code></p> <p>Optional execution backend for custom parallelism.</p> <p><code>run_id</code> : <code>str | None = None</code></p> <p>Explicit run identifier. If <code>None</code>, one is generated automatically.</p> <p><code>resume</code> : <code>bool = True</code></p> <p>If <code>True</code>, reuse cached results when available.</p> <p><code>on_result</code> : <code>Callable[[GenerationRecord], None] | None = None</code></p> <p>Callback invoked per generation record.</p> <p><code>**kwargs</code> : <code>Any</code></p> <p>Currently used for <code>top_p</code> in sampling. Other provider-specific kwargs are reserved for future wiring.</p>"},{"location":"api/evaluate/#return-value","title":"Return Value","text":"<p><code>ExperimentReport</code> containing: - <code>generation_results</code>: list of <code>GenerationRecord</code> - <code>evaluation_report</code>: <code>EvaluationReport</code> with aggregates and per-sample scores - <code>failures</code>: generation failures - <code>metadata</code>: run metadata</p> <p>Access aggregate metrics via:</p> <pre><code>report.evaluation_report.metrics[\"ExactMatch\"].mean\n</code></pre>"},{"location":"api/evaluate/#examples","title":"Examples","text":""},{"location":"api/evaluate/#basic-benchmark","title":"Basic benchmark","text":"<pre><code>from themis import evaluate\n\nreport = evaluate(\n    \"gsm8k\",\n    model=\"gpt-4\",\n    limit=100,\n)\n\naccuracy = report.evaluation_report.metrics[\"ExactMatch\"].mean\nprint(f\"Accuracy: {accuracy:.2%}\")\n</code></pre>"},{"location":"api/evaluate/#custom-dataset","title":"Custom dataset","text":"<pre><code>dataset = [\n    {\"id\": \"1\", \"question\": \"2+2\", \"answer\": \"4\"},\n    {\"id\": \"2\", \"question\": \"3+3\", \"answer\": \"6\"},\n]\n\nreport = evaluate(\n    dataset,\n    model=\"gpt-4\",\n    prompt=\"Q: {question}\\nA:\",\n    metrics=[\"exact_match\"],\n)\n</code></pre>"},{"location":"api/evaluate/#advanced-storage-execution","title":"Advanced storage + execution","text":"<pre><code>from themis import evaluate\nfrom themis.backends.execution import LocalExecutionBackend\nfrom themis.backends.storage import LocalFileStorageBackend\n\nreport = evaluate(\n    \"math500\",\n    model=\"gpt-4\",\n    storage_backend=LocalFileStorageBackend(\".cache/experiments\"),\n    execution_backend=LocalExecutionBackend(max_workers=8),\n)\n</code></pre>"},{"location":"api/evaluate/#exporting-results","title":"Exporting results","text":"<p><code>evaluate()</code> does not export files directly. Use export helpers:</p> <pre><code>from pathlib import Path\nfrom themis.experiment import export\n\nreport = evaluate(\"gsm8k\", model=\"gpt-4\", limit=50)\nexport.export_report_json(report, Path(\"report.json\"))\nexport.export_report_csv(report, Path(\"report.csv\"))\nexport.export_html_report(report, Path(\"report.html\"))\n</code></pre>"},{"location":"api/metrics/","title":"Metrics API","text":"<p>Evaluation metrics for different domains.</p>"},{"location":"api/metrics/#overview","title":"Overview","text":"<p>Themis includes metrics for: - Core: <code>ExactMatch</code>, <code>ResponseLength</code> - Math: <code>MathVerifyAccuracy</code> - NLP: <code>BLEU</code>, <code>ROUGE</code>, <code>BERTScore</code>, <code>METEOR</code> - Code: <code>PassAtK</code>, <code>ExecutionAccuracy</code>, <code>CodeBLEU</code></p> <p>Metric names used in reports are taken from each metric's <code>name</code> attribute (e.g., <code>\"ExactMatch\"</code>).</p>"},{"location":"api/metrics/#dependencies-and-optional-extras","title":"Dependencies and optional extras","text":"<ul> <li>MathVerifyAccuracy requires <code>themis-eval[math]</code> (package: <code>math-verify</code>).</li> <li>NLP metrics require <code>themis-eval[nlp]</code> (<code>sacrebleu</code>, <code>rouge-score</code>, <code>bert-score</code>, <code>nltk</code>).</li> <li>CodeBLEU requires <code>themis-eval[code]</code> (package: <code>codebleu</code>).</li> </ul> <p>Install extras: <pre><code>pip install themis-eval[math,nlp,code]\n</code></pre></p>"},{"location":"api/metrics/#base-interface","title":"Base Interface","text":""},{"location":"api/metrics/#metric","title":"Metric","text":"<p>All metrics implement <code>themis.interfaces.Metric</code>:</p> <pre><code>from themis.core.entities import MetricScore\nfrom themis.interfaces import Metric\n\nclass CustomMetric(Metric):\n    name = \"MyMetric\"\n\n    def compute(self, *, prediction, references, metadata=None) -&gt; MetricScore:\n        score = 1.0 if prediction in references else 0.0\n        return MetricScore(metric_name=self.name, value=score)\n</code></pre> <p>Notes: - <code>prediction</code> is already extracted by the pipeline's extractor. - <code>references</code> is a list of normalized reference values.</p>"},{"location":"api/metrics/#core-metrics","title":"Core Metrics","text":""},{"location":"api/metrics/#exactmatch","title":"ExactMatch","text":"<p>Exact string matching after normalization.</p> <pre><code>from themis.evaluation.metrics import ExactMatch\n\nmetric = ExactMatch()\nscore = metric.compute(prediction=\"4\", references=[\"4\"])\nprint(score.value)  # 1.0\n</code></pre>"},{"location":"api/metrics/#responselength","title":"ResponseLength","text":"<p>Length-based metric useful for detecting verbosity or truncation.</p> <pre><code>from themis.evaluation.metrics import ResponseLength\n\nmetric = ResponseLength()\nscore = metric.compute(prediction=\"short answer\", references=[\"\"])\nprint(score.value)\n</code></pre>"},{"location":"api/metrics/#math-metrics","title":"Math Metrics","text":""},{"location":"api/metrics/#mathverifyaccuracy","title":"MathVerifyAccuracy","text":"<p>Symbolic and numeric math verification.</p> <pre><code>from themis.evaluation.metrics import MathVerifyAccuracy\n\nmetric = MathVerifyAccuracy()\nscore = metric.compute(prediction=\"2.0\", references=[\"2\"])\nprint(score.value)  # 1.0\n</code></pre>"},{"location":"api/metrics/#nlp-metrics","title":"NLP Metrics","text":"<p>Requires: <code>pip install themis-eval[nlp]</code></p>"},{"location":"api/metrics/#bleu","title":"BLEU","text":"<pre><code>from themis.evaluation.metrics.nlp import BLEU\n\nmetric = BLEU()\nscore = metric.compute(prediction=\"The cat is on the mat\", references=[\"The cat is on the mat\"])\nprint(score.value)\n</code></pre>"},{"location":"api/metrics/#rouge","title":"ROUGE","text":"<pre><code>from themis.evaluation.metrics.nlp import ROUGE, ROUGEVariant\n\nmetric = ROUGE(variant=ROUGEVariant.ROUGE_L)\nscore = metric.compute(prediction=\"A\", references=[\"A\"])\nprint(score.value)\n</code></pre>"},{"location":"api/metrics/#bertscore","title":"BERTScore","text":"<pre><code>from themis.evaluation.metrics.nlp import BERTScore\n\nmetric = BERTScore()\nscore = metric.compute(prediction=\"The cat sits on the mat\", references=[\"A feline rests on the rug\"])\nprint(score.value)\n</code></pre>"},{"location":"api/metrics/#meteor","title":"METEOR","text":"<pre><code>from themis.evaluation.metrics.nlp import METEOR\n\nmetric = METEOR()\nscore = metric.compute(prediction=\"A\", references=[\"A\"])\n</code></pre>"},{"location":"api/metrics/#code-metrics","title":"Code Metrics","text":"<p>Requires: <code>pip install themis-eval[code]</code></p>"},{"location":"api/metrics/#passatk","title":"PassAtK","text":"<pre><code>from themis.evaluation.metrics.code.pass_at_k import PassAtK\n\nmetric = PassAtK(k=10)\nscore = metric.compute(prediction=\"code\", references=[\"expected\"])\n</code></pre>"},{"location":"api/metrics/#executionaccuracy","title":"ExecutionAccuracy","text":"<pre><code>from themis.evaluation.metrics.code.execution import ExecutionAccuracy\n\nmetric = ExecutionAccuracy()\nscore = metric.compute(prediction=\"code\", references=[\"expected\"])\n</code></pre>"},{"location":"api/metrics/#codebleu","title":"CodeBLEU","text":"<pre><code>from themis.evaluation.metrics.code.codebleu import CodeBLEU\n\nmetric = CodeBLEU()\nscore = metric.compute(prediction=\"code\", references=[\"expected\"])\n</code></pre>"},{"location":"api/metrics/#using-metrics-in-evaluate","title":"Using metrics in <code>evaluate()</code>","text":"<pre><code>from themis import evaluate\n\nreport = evaluate(\n    \"gsm8k\",\n    model=\"gpt-4\",\n    metrics=[\"exact_match\", \"math_verify\"],\n)\n</code></pre>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>Complete API documentation for Themis.</p>"},{"location":"api/overview/#core-api","title":"Core API","text":""},{"location":"api/overview/#main-functions","title":"Main Functions","text":"<p>The primary entry point for running evaluations. See evaluate() documentation for details.</p>"},{"location":"api/overview/#themis.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    benchmark_or_dataset: str | Sequence[dict[str, Any]],\n    *,\n    model: str,\n    limit: int | None = None,\n    prompt: str | None = None,\n    metrics: list[str] | None = None,\n    temperature: float = 0.0,\n    max_tokens: int = 512,\n    num_samples: int = 1,\n    distributed: bool = False,\n    workers: int = 4,\n    storage: str | Path | None = None,\n    storage_backend: object | None = None,\n    execution_backend: object | None = None,\n    run_id: str | None = None,\n    resume: bool = True,\n    on_result: Callable[[GenerationRecord], None] | None = None,\n    **kwargs: Any,\n) -&gt; ExperimentReport\n</code></pre> <p>Run an LLM evaluation with automatic configuration.</p> <p>This is the primary API for Themis. It auto-configures prompts, metrics, and extractors based on the benchmark name, or allows full customization for custom datasets.</p> PARAMETER DESCRIPTION <code>benchmark_or_dataset</code> <p>Either a benchmark name (e.g., \"math500\", \"gsm8k\") or a list of dataset samples as dictionaries. For custom datasets, each dict should have: prompt/question (input), answer/reference (output), and optionally id (unique identifier).</p> <p> TYPE: <code>str | Sequence[dict[str, Any]]</code> </p> <code>model</code> <p>Model identifier for LiteLLM (e.g., \"gpt-4\", \"claude-3-opus-20240229\", \"azure/gpt-4\", \"ollama/llama3\"). Provider is auto-detected from the name.</p> <p> TYPE: <code>str</code> </p> <code>limit</code> <p>Maximum number of samples to evaluate. Use for testing or when you want to evaluate a subset. None means evaluate all samples.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>prompt</code> <p>Custom prompt template using Python format strings. Variables like {prompt}, {question}, {context} will be replaced with dataset fields. If None, uses the benchmark's default prompt template.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>List of metric names to compute. Available: \"ExactMatch\", \"MathVerify\", \"BLEU\", \"ROUGE\", \"BERTScore\", \"METEOR\", \"PassAtK\", \"CodeBLEU\", \"ExecutionAccuracy\". If None, uses benchmark defaults.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Sampling temperature (0.0 = deterministic/greedy, 1.0 = standard, 2.0 = very random). Recommended: 0.0 for evaluation reproducibility.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>max_tokens</code> <p>Maximum tokens in model response. Typical values: 256 for short answers, 512 for medium, 2048 for long explanations or code.</p> <p> TYPE: <code>int</code> DEFAULT: <code>512</code> </p> <code>num_samples</code> <p>Number of responses to generate per prompt. Use &gt;1 for Pass@K metrics, ensembling, or measuring response variance.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>distributed</code> <p>Whether to use distributed execution. Currently a placeholder for future Ray integration.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>workers</code> <p>Number of parallel workers for generation. Higher = faster but may hit rate limits. Recommended: 4-16 for APIs, 32+ for local models.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>storage</code> <p>Storage location for results and cache. Defaults to \".cache/experiments\". Can be a local path or (future) cloud storage URI.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>storage_backend</code> <p>Optional storage backend instance. Typically an ExperimentStorage or LocalFileStorageBackend (adapter). Custom storage backends are not yet integrated with the evaluate() API.</p> <p> TYPE: <code>object | None</code> DEFAULT: <code>None</code> </p> <code>execution_backend</code> <p>Optional execution backend for custom parallelism.</p> <p> TYPE: <code>object | None</code> DEFAULT: <code>None</code> </p> <code>run_id</code> <p>Unique identifier for this run. If None, auto-generated from timestamp (e.g., \"run-2024-01-15-123456\"). Use meaningful IDs for tracking experiments.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>resume</code> <p>Whether to resume from cached results.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_result</code> <p>Optional callback function called for each result.</p> <p> TYPE: <code>Callable[[GenerationRecord], None] | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional provider-specific options.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ExperimentReport</code> <p>ExperimentReport containing generation results, evaluation metrics,</p> <code>ExperimentReport</code> <p>and metadata.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If benchmark is unknown or configuration is invalid.</p> <code>RuntimeError</code> <p>If evaluation fails.</p> Example <p>report = themis.evaluate(\"math500\", model=\"gpt-4\", limit=10) print(f\"Accuracy: {report.evaluation_report.metrics['accuracy']:.2%}\") Accuracy: 85.00%</p>"},{"location":"api/overview/#sessions-specs","title":"Sessions &amp; Specs","text":"<ul> <li><code>ExperimentSession</code> - Orchestrates runs with explicit specs</li> <li><code>ExperimentSpec</code> - Dataset + prompt + model + pipeline</li> <li><code>ExecutionSpec</code> - Execution backend + worker config</li> <li><code>StorageSpec</code> - Storage backend + cache config</li> </ul> <p>See Session Reference and Specs Reference.</p>"},{"location":"api/overview/#modules","title":"Modules","text":""},{"location":"api/overview/#comparison","title":"Comparison","text":"<p>Statistical comparison of multiple runs.</p> <ul> <li>compare_runs() - Compare experiment runs</li> <li>Statistical Tests - T-test, bootstrap, permutation</li> <li>Reports - ComparisonReport, WinLossMatrix</li> </ul>"},{"location":"api/overview/#presets","title":"Presets","text":"<p>Built-in benchmark configurations.</p> <ul> <li>list_benchmarks() - List available benchmarks</li> <li>get_benchmark_preset() - Get benchmark configuration</li> <li>BenchmarkPreset - Preset data structure</li> </ul>"},{"location":"api/overview/#metrics","title":"Metrics","text":"<p>Evaluation metrics for different domains.</p> <ul> <li>Core: ExactMatch, ResponseLength</li> <li>Math: MathVerifyAccuracy</li> <li>NLP: BLEU, ROUGE, BERTScore, METEOR</li> <li>Code: PassAtK, CodeBLEU, ExecutionAccuracy</li> </ul>"},{"location":"api/overview/#backends","title":"Backends","text":"<p>Pluggable backend interfaces.</p> <ul> <li>Storage: <code>StorageBackend</code>, <code>LocalFileStorageBackend</code></li> <li>Execution: <code>ExecutionBackend</code>, <code>LocalExecutionBackend</code>, <code>SequentialExecutionBackend</code></li> </ul>"},{"location":"api/overview/#package-structure","title":"Package Structure","text":"<pre><code>themis/\n\u251c\u2500\u2500 __init__.py              # Main exports\n\u251c\u2500\u2500 api.py                   # evaluate() function\n\u251c\u2500\u2500 session.py               # ExperimentSession\n\u251c\u2500\u2500 specs/                   # ExperimentSpec, ExecutionSpec, StorageSpec\n\u251c\u2500\u2500 presets/                 # Benchmark presets\n\u2502   \u251c\u2500\u2500 benchmarks.py\n\u2502   \u2514\u2500\u2500 models.py\n\u251c\u2500\u2500 comparison/              # Statistical comparison\n\u2502   \u251c\u2500\u2500 engine.py\n\u2502   \u251c\u2500\u2500 statistics.py\n\u2502   \u2514\u2500\u2500 reports.py\n\u251c\u2500\u2500 evaluation/              # Metrics &amp; evaluation\n\u2502   \u2514\u2500\u2500 metrics/\n\u251c\u2500\u2500 backends/                # Pluggable backends\n\u2502   \u251c\u2500\u2500 storage.py\n\u2502   \u2514\u2500\u2500 execution.py\n\u251c\u2500\u2500 generation/              # LLM generation\n\u251c\u2500\u2500 experiment/              # Experiment orchestration\n\u251c\u2500\u2500 storage/                 # Storage adapters\n\u251c\u2500\u2500 server/                  # API server\n\u2514\u2500\u2500 cli/                     # Command-line interface\n</code></pre>"},{"location":"api/overview/#type-hints","title":"Type Hints","text":"<p>Themis is fully typed with Python type hints:</p> <pre><code>from themis import evaluate\nfrom themis.core.entities import ExperimentReport\n\nreport: ExperimentReport = evaluate(\n    \"gsm8k\",\n    model=\"gpt-4\",\n)\n</code></pre>"},{"location":"api/presets/","title":"Presets API","text":"<p>Built-in benchmark configurations.</p>"},{"location":"api/presets/#functions","title":"Functions","text":""},{"location":"api/presets/#list_benchmarks","title":"list_benchmarks()","text":"<p>List all available benchmark presets.</p> <pre><code>from themis.presets import list_benchmarks\n\nbenchmarks = list_benchmarks()\nprint(benchmarks)\n# ['demo', 'gsm8k', 'math500', 'aime24', ...]\n</code></pre> <p>Returns: <code>list[str]</code> - Benchmark names</p>"},{"location":"api/presets/#get_benchmark_preset","title":"get_benchmark_preset()","text":"<p>Get configuration for a specific benchmark.</p> <pre><code>from themis.presets import get_benchmark_preset\n\npreset = get_benchmark_preset(\"gsm8k\")\n</code></pre> <p>Parameters: - <code>name</code> : <code>str</code> - Benchmark name</p> <p>Returns: <code>BenchmarkPreset</code></p> <p>Raises: <code>ValueError</code> if benchmark not found</p>"},{"location":"api/presets/#classes","title":"Classes","text":""},{"location":"api/presets/#benchmarkpreset","title":"BenchmarkPreset","text":"<p>Complete benchmark configuration.</p> <p>Attributes: - <code>name</code> : <code>str</code> - Benchmark identifier - <code>prompt_template</code> : <code>PromptTemplate</code> - Prompt formatting - <code>metrics</code> : <code>list[Metric]</code> - Metric instances - <code>extractor</code> : <code>Extractor</code> - Output extractor - <code>dataset_loader</code> : <code>Callable</code> - Dataset loading function - <code>metadata_fields</code> : <code>tuple[str, ...]</code> - Fields to preserve - <code>reference_field</code> : <code>str</code> - Field containing reference answer - <code>dataset_id_field</code> : <code>str</code> - Field containing sample ID - <code>description</code> : <code>str</code> - Human-readable description</p>"},{"location":"api/presets/#example","title":"Example","text":"<pre><code>from themis.evaluation import extractors, metrics\nfrom themis.generation.templates import PromptTemplate\nfrom themis.presets.benchmarks import BenchmarkPreset, register_benchmark\n\n\ndef my_dataset_loader(limit=None):\n    data = [\n        {\"id\": \"1\", \"question\": \"2+2\", \"answer\": \"4\"},\n        {\"id\": \"2\", \"question\": \"3+3\", \"answer\": \"6\"},\n    ]\n    return data[:limit] if limit else data\n\n\npreset = BenchmarkPreset(\n    name=\"my-benchmark\",\n    prompt_template=PromptTemplate(name=\"my-template\", template=\"Q: {question}\\nA:\"),\n    metrics=[metrics.ExactMatch()],\n    extractor=extractors.IdentityExtractor(),\n    dataset_loader=my_dataset_loader,\n    description=\"My custom benchmark\",\n)\n\nregister_benchmark(preset)\n</code></pre>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":"<p>Understanding the key concepts in Themis will help you use it effectively.</p>"},{"location":"getting-started/concepts/#architecture-overview","title":"Architecture Overview","text":"<p>Themis is built on a layered architecture:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           themis.evaluate()          \u2502  Simple API\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        ExperimentSession     \u2502  spec-driven orchestration\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502   ExperimentSpec   \u2502  Dataset + prompt + model\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   EvaluationPipelineContract \u2502    \u2502    StorageSpec        \u2502\n\u2502   (extractor + metrics)       \u2502    \u2502    + ExperimentStorage\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Generation  \u2502  Providers + execution backend\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/concepts/#1-evaluation","title":"1. Evaluation","text":"<p>Evaluation is the process of testing an LLM on a dataset and computing metrics.</p> <pre><code>from themis import evaluate\n\nreport = evaluate(\n    \"gsm8k\",\n    model=\"gpt-4\",\n    metrics=[\"exact_match\"],\n)\n\naccuracy = report.evaluation_report.metrics[\"ExactMatch\"].mean\n</code></pre> <p>Three steps: 1. Generation: LLM produces responses 2. Evaluation: Metrics compare responses to references 3. Reporting: Results are aggregated and stored</p>"},{"location":"getting-started/concepts/#2-benchmarks-presets","title":"2. Benchmarks &amp; Presets","text":"<p>Benchmarks are standardized evaluation datasets with: - Prompts/questions - Reference answers - Evaluation metrics - Prompt templates</p> <p>Presets package these into ready-to-use configurations:</p> <pre><code>evaluate(\"gsm8k\", model=\"gpt-4\")\nevaluate(\"mmlu-pro\", model=\"gpt-4\")\nevaluate(\"aime24\", model=\"gpt-4\")\n</code></pre> <p>Each preset includes: - Dataset loader - Default prompt template - Appropriate metrics - Reference field mapping</p>"},{"location":"getting-started/concepts/#3-metrics","title":"3. Metrics","text":"<p>Metrics quantify how well an LLM performs:</p> <p>Core Metrics: - <code>ExactMatch</code> - Exact string matching - <code>ResponseLength</code> - Length-based signal</p> <p>Math Metrics: - <code>MathVerifyAccuracy</code> - Symbolic &amp; numeric verification</p> <p>NLP Metrics: - <code>BLEU</code>, <code>ROUGE</code>, <code>BERTScore</code>, <code>METEOR</code></p> <p>Code Metrics: - <code>PassAtK</code>, <code>ExecutionAccuracy</code>, <code>CodeBLEU</code></p> <pre><code>report = evaluate(\n    \"gsm8k\",\n    model=\"gpt-4\",\n    metrics=[\"exact_match\", \"math_verify\"],\n)\n\nprint(report.evaluation_report.metrics)\n</code></pre>"},{"location":"getting-started/concepts/#4-storage-caching","title":"4. Storage &amp; Caching","text":"<p>Themis caches results to enable: - Resuming failed runs - Avoiding duplicate API calls - Reproducibility</p> <pre><code># First run - generates responses\nreport1 = evaluate(\"gsm8k\", model=\"gpt-4\", run_id=\"my-exp\")\n\n# Second run - uses cache\nreport2 = evaluate(\n    \"gsm8k\",\n    model=\"gpt-4\",\n    run_id=\"my-exp\",\n    resume=True,\n)\n</code></pre> <p>Under the hood, caching uses <code>ExperimentStorage</code> and <code>StorageSpec</code>.</p>"},{"location":"getting-started/concepts/#5-comparison","title":"5. Comparison","text":"<p>Comparison analyzes differences between runs with statistical rigor:</p> <pre><code>from themis.comparison import compare_runs\n\nreport = compare_runs(\n    run_ids=[\"gpt4-run\", \"claude-run\"],\n    storage_path=\".cache/experiments\",\n)\n\nprint(report.summary())\n</code></pre>"},{"location":"getting-started/concepts/#6-providers","title":"6. Providers","text":"<p>Themis uses LiteLLM for provider support:</p> <pre><code># OpenAI\nevaluate(\"gsm8k\", model=\"gpt-4\")\n\n# Anthropic\nevaluate(\"gsm8k\", model=\"claude-3-opus-20240229\")\n\n# Azure OpenAI\nevaluate(\"gsm8k\", model=\"azure/gpt-4\")\n</code></pre>"},{"location":"getting-started/concepts/#7-specs-sessions","title":"7. Specs &amp; Sessions","text":"<p>For advanced control, use explicit specs:</p> <pre><code>from themis.evaluation.pipeline import MetricPipeline\nfrom themis.evaluation import extractors, metrics\nfrom themis.session import ExperimentSession\nfrom themis.specs import ExperimentSpec, ExecutionSpec, StorageSpec\n\npipeline = MetricPipeline(\n    extractor=extractors.IdentityExtractor(),\n    metrics=[metrics.ResponseLength()],\n)\n\nspec = ExperimentSpec(\n    dataset=[{\"id\": \"1\", \"question\": \"2+2\", \"answer\": \"4\"}],\n    prompt=\"Solve: {question}\",\n    model=\"fake:fake-math-llm\",\n    sampling={\"temperature\": 0.0, \"max_tokens\": 128},\n    pipeline=pipeline,\n)\n\nreport = ExperimentSession().run(\n    spec,\n    execution=ExecutionSpec(workers=2),\n    storage=StorageSpec(path=\".cache/experiments\"),\n)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":"<pre><code>pip install themis-eval\n</code></pre> <p>With optional extras:</p> <pre><code>pip install \"themis-eval[math,nlp,code,server]\"\n</code></pre> <p>Using <code>uv</code>:</p> <pre><code>uv pip install themis-eval\nuv pip install \"themis-eval[math,nlp,code,server]\"\n</code></pre>"},{"location":"getting-started/installation/#contributor-environment-recommended","title":"Contributor Environment (Recommended)","text":"<p>For local development and full validation (tests + docs + server + metrics extras):</p> <pre><code>uv sync --extra dev --extra docs --extra server --extra nlp --extra code --extra math\n</code></pre>"},{"location":"getting-started/installation/#verify","title":"Verify","text":"<pre><code>themis demo --limit 3\n</code></pre> <p>Or Python:</p> <pre><code>from themis import evaluate\n\nreport = evaluate(\"demo\", model=\"fake-math-llm\", limit=3)\nprint(len(report.generation_results))\n</code></pre> <p>Validate docs build:</p> <pre><code>uv run python -m mkdocs build --strict\n</code></pre>"},{"location":"getting-started/installation/#optional-provider-setup","title":"Optional Provider Setup","text":"<p>Set provider keys only when using real hosted models:</p> <pre><code>export OPENAI_API_KEY=\"...\"\nexport ANTHROPIC_API_KEY=\"...\"\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#1-run-your-first-evaluation","title":"1) Run Your First Evaluation","text":"<pre><code>from themis import evaluate\n\nreport = evaluate(\"gsm8k\", model=\"gpt-4\", limit=10)\naccuracy = report.evaluation_report.metrics[\"ExactMatch\"].mean\nprint(f\"Accuracy: {accuracy:.2%}\")\n</code></pre>"},{"location":"getting-started/quickstart/#2-use-specs-session","title":"2) Use Specs + Session","text":"<pre><code>from themis.evaluation.metric_pipeline import MetricPipeline\nfrom themis.presets import get_benchmark_preset\nfrom themis.session import ExperimentSession\nfrom themis.specs import ExecutionSpec, ExperimentSpec, StorageSpec\n\npreset = get_benchmark_preset(\"demo\")\npipeline = MetricPipeline(extractor=preset.extractor, metrics=preset.metrics)\n\nspec = ExperimentSpec(\n    dataset=preset.load_dataset(limit=5),\n    prompt=preset.prompt_template.template,\n    model=\"fake:fake-math-llm\",\n    sampling={\"temperature\": 0.0, \"max_tokens\": 128},\n    pipeline=pipeline,\n)\n\nreport = ExperimentSession().run(\n    spec,\n    execution=ExecutionSpec(workers=2),\n    storage=StorageSpec(path=\".cache/experiments\"),\n)\n</code></pre>"},{"location":"getting-started/quickstart/#3-cli-flow","title":"3) CLI Flow","text":"<pre><code>themis eval gsm8k --model gpt-4 --limit 100 --run-id run-a\nthemis eval gsm8k --model gpt-4 --temperature 0.7 --limit 100 --run-id run-b\nthemis compare run-a run-b\n</code></pre>"},{"location":"getting-started/quickstart/#4-explore-examples","title":"4) Explore Examples","text":"<ul> <li><code>examples-simple/01_quickstart.py</code></li> <li><code>examples-simple/02_custom_dataset.py</code></li> <li><code>examples-simple/04_comparison.py</code></li> <li><code>examples-simple/07_provider_ready.py</code></li> <li><code>examples-simple/08_resume_cache.py</code></li> <li><code>examples-simple/09_research_loop.py</code></li> </ul>"},{"location":"guides/cli/","title":"CLI Reference","text":"<p>Complete guide to the Themis command-line interface.</p>"},{"location":"guides/cli/#overview","title":"Overview","text":"<p>Themis provides a focused set of commands:</p> <pre><code>themis demo     # Run the demo benchmark\nthemis eval     # Run evaluations\nthemis compare  # Compare runs statistically\nthemis share    # Generate shareable assets\nthemis serve    # Start API server\nthemis list     # List runs, benchmarks, metrics\nthemis clean    # Clean old runs\n</code></pre> <p>Defaults for storage can be set with <code>THEMIS_STORAGE</code>.</p>"},{"location":"guides/cli/#themis-demo","title":"themis demo","text":"<p>Run the built-in demo benchmark.</p> <pre><code>themis demo --model fake-math-llm --limit 10\n</code></pre>"},{"location":"guides/cli/#themis-eval","title":"themis eval","text":"<p>Run an evaluation on a benchmark.</p>"},{"location":"guides/cli/#synopsis","title":"Synopsis","text":"<pre><code>themis eval BENCHMARK --model MODEL [OPTIONS]\n</code></pre>"},{"location":"guides/cli/#options","title":"Options","text":"<ul> <li><code>--model MODEL</code> (required)</li> <li><code>--limit N</code></li> <li><code>--prompt TEMPLATE</code></li> <li><code>--temperature FLOAT</code></li> <li><code>--max-tokens INT</code></li> <li><code>--workers INT</code></li> <li><code>--run-id STR</code></li> <li><code>--storage PATH</code></li> <li><code>--resume / --no-resume</code></li> <li><code>--output FILE</code> (<code>.csv</code>, <code>.json</code>, <code>.html</code>)</li> <li><code>--distributed</code> (not yet supported)</li> </ul> <p>Notes: - Custom dataset files are not yet supported via CLI. Use the Python API for custom datasets. - <code>--distributed</code> currently returns an error in the current CLI.</p>"},{"location":"guides/cli/#examples","title":"Examples","text":"<pre><code># Basic evaluation\nthemis eval gsm8k --model gpt-4\n\n# Limit to 100 samples\nthemis eval gsm8k --model gpt-4 --limit 100\n\n# Custom prompt\nthemis eval gsm8k --model gpt-4 --prompt \"Q: {prompt}\\nA:\"\n\n# Export results\nthemis eval gsm8k --model gpt-4 --output results.json\n</code></pre>"},{"location":"guides/cli/#themis-compare","title":"themis compare","text":"<p>Compare multiple runs with statistical tests.</p>"},{"location":"guides/cli/#synopsis_1","title":"Synopsis","text":"<pre><code>themis compare RUN_ID_1 RUN_ID_2 [RUN_ID_3...] [OPTIONS]\n</code></pre>"},{"location":"guides/cli/#options_1","title":"Options","text":"<ul> <li><code>--metric NAME</code> (limit comparison to one metric)</li> <li><code>--storage PATH</code></li> <li><code>--output FILE</code> (<code>.json</code>, <code>.html</code>, <code>.md</code>)</li> <li><code>--show-diff</code></li> </ul>"},{"location":"guides/cli/#example","title":"Example","text":"<pre><code>themis compare run-1 run-2 --output comparison.html --show-diff\n</code></pre>"},{"location":"guides/cli/#themis-share","title":"themis share","text":"<p>Generate a shareable SVG badge and Markdown snippet for a run.</p>"},{"location":"guides/cli/#synopsis_2","title":"Synopsis","text":"<pre><code>themis share RUN_ID [OPTIONS]\n</code></pre>"},{"location":"guides/cli/#options_2","title":"Options","text":"<ul> <li><code>--metric NAME</code> (highlight metric)</li> <li><code>--storage PATH</code></li> <li><code>--output-dir DIR</code></li> </ul>"},{"location":"guides/cli/#example_1","title":"Example","text":"<pre><code>themis share run-20260118-032014 --metric accuracy --output-dir share\n</code></pre>"},{"location":"guides/cli/#themis-serve","title":"themis serve","text":"<p>Start the API server with REST and WebSocket endpoints.</p>"},{"location":"guides/cli/#synopsis_3","title":"Synopsis","text":"<pre><code>themis serve [OPTIONS]\n</code></pre>"},{"location":"guides/cli/#options_3","title":"Options","text":"<ul> <li><code>--port INT</code> (default: 8080)</li> <li><code>--host STR</code> (default: 127.0.0.1)</li> <li><code>--storage PATH</code></li> <li><code>--reload</code> (dev mode)</li> </ul> <p>Requires <code>themis[server]</code>.</p>"},{"location":"guides/cli/#themis-list","title":"themis list","text":"<p>List runs, benchmarks, or metrics.</p>"},{"location":"guides/cli/#synopsis_4","title":"Synopsis","text":"<pre><code>themis list WHAT [OPTIONS]\n</code></pre>"},{"location":"guides/cli/#options_4","title":"Options","text":"<ul> <li><code>runs</code> | <code>benchmarks</code> | <code>metrics</code></li> <li><code>--storage PATH</code></li> <li><code>--limit N</code></li> <li><code>--verbose</code></li> </ul>"},{"location":"guides/cli/#examples_1","title":"Examples","text":"<pre><code>themis list benchmarks\nthemis list metrics\nthemis list runs --verbose\n</code></pre>"},{"location":"guides/cli/#themis-clean","title":"themis clean","text":"<p>Clean runs older than a threshold.</p>"},{"location":"guides/cli/#synopsis_5","title":"Synopsis","text":"<pre><code>themis clean --older-than DAYS [OPTIONS]\n</code></pre>"},{"location":"guides/cli/#options_5","title":"Options","text":"<ul> <li><code>--storage PATH</code></li> <li><code>--older-than DAYS</code></li> <li><code>--dry-run</code></li> </ul>"},{"location":"guides/cli/#example_2","title":"Example","text":"<pre><code># Preview what will be deleted\nthemis clean --older-than 30 --dry-run\n\n# Delete runs older than 30 days\nthemis clean --older-than 30\n</code></pre>"},{"location":"guides/cli/#global-options","title":"Global Options","text":"<ul> <li><code>--help</code></li> <li><code>--version</code></li> </ul>"},{"location":"guides/cli/#environment-variables","title":"Environment Variables","text":""},{"location":"guides/cli/#api-keys","title":"API Keys","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"guides/cli/#themis-settings","title":"Themis Settings","text":"<pre><code># Default storage\nexport THEMIS_STORAGE=\"~/.themis/experiments\"\n\n# Log level\nexport THEMIS_LOG_LEVEL=\"INFO\"\n</code></pre>"},{"location":"guides/comparison/","title":"Comparison Guide","text":"<p>Use <code>themis.comparison.compare_runs</code> to compare two or more run IDs.</p>"},{"location":"guides/comparison/#basic","title":"Basic","text":"<pre><code>from themis.comparison import compare_runs\nfrom themis.comparison.statistics import StatisticalTest\n\nreport = compare_runs(\n    run_ids=[\"run-a\", \"run-b\"],\n    storage_path=\".cache/experiments\",\n    statistical_test=StatisticalTest.BOOTSTRAP,\n    alpha=0.05,\n)\n\nprint(report.summary())\n</code></pre>"},{"location":"guides/comparison/#metric-scoped-comparison","title":"Metric-Scoped Comparison","text":"<pre><code>report = compare_runs(\n    run_ids=[\"run-a\", \"run-b\", \"run-c\"],\n    storage_path=\".cache/experiments\",\n    metrics=[\"ExactMatch\"],\n)\n</code></pre>"},{"location":"guides/comparison/#cli","title":"CLI","text":"<pre><code>themis compare run-a run-b --output comparison.html\n</code></pre>"},{"location":"guides/comparison/#notes","title":"Notes","text":"<ul> <li>Run IDs must exist in the same storage root.</li> <li>Metrics compared are the intersection available across selected runs.</li> </ul>"},{"location":"guides/evaluation/","title":"Evaluation Guide","text":""},{"location":"guides/evaluation/#api-levels","title":"API Levels","text":"<p>Themis supports two evaluation entry points:</p> <ul> <li><code>themis.evaluate(...)</code>: fastest path for common workflows.</li> <li><code>ExperimentSession().run(spec, ...)</code>: explicit spec/session API with full control.</li> </ul>"},{"location":"guides/evaluation/#quick-api","title":"Quick API","text":"<pre><code>from themis import evaluate\n\nreport = evaluate(\n    \"gsm8k\",\n    model=\"gpt-4\",\n    limit=100,\n    metrics=[\"exact_match\", \"math_verify\"],\n)\n</code></pre>"},{"location":"guides/evaluation/#spec-api","title":"Spec API","text":"<pre><code>from themis.evaluation.metric_pipeline import MetricPipeline\nfrom themis.presets import get_benchmark_preset\nfrom themis.session import ExperimentSession\nfrom themis.specs import ExecutionSpec, ExperimentSpec, StorageSpec\n\npreset = get_benchmark_preset(\"gsm8k\")\npipeline = MetricPipeline(extractor=preset.extractor, metrics=preset.metrics)\n\nspec = ExperimentSpec(\n    dataset=preset.load_dataset(limit=100),\n    prompt=preset.prompt_template.template,\n    model=\"litellm:gpt-4\",\n    sampling={\"temperature\": 0.0, \"max_tokens\": 512},\n    pipeline=pipeline,\n    run_id=\"gsm8k-gpt4\",\n)\n\nreport = ExperimentSession().run(\n    spec,\n    execution=ExecutionSpec(workers=8),\n    storage=StorageSpec(path=\".cache/experiments\", cache=True),\n)\n</code></pre>"},{"location":"guides/evaluation/#custom-dataset","title":"Custom Dataset","text":"<pre><code>from themis.evaluation import extractors, metrics\nfrom themis.evaluation.metric_pipeline import MetricPipeline\nfrom themis.session import ExperimentSession\nfrom themis.specs import ExperimentSpec\n\ndataset = [\n    {\"id\": \"1\", \"question\": \"2+2\", \"answer\": \"4\"},\n    {\"id\": \"2\", \"question\": \"3+3\", \"answer\": \"6\"},\n]\n\npipeline = MetricPipeline(\n    extractor=extractors.IdentityExtractor(),\n    metrics=[metrics.ExactMatch(), metrics.ResponseLength()],\n)\n\nspec = ExperimentSpec(\n    dataset=dataset,\n    prompt=\"Q: {question}\\nA:\",\n    model=\"fake:fake-math-llm\",\n    pipeline=pipeline,\n)\n\nreport = ExperimentSession().run(spec)\n</code></pre>"},{"location":"guides/evaluation/#reading-results","title":"Reading Results","text":"<pre><code>for name, aggregate in report.evaluation_report.metrics.items():\n    print(name, aggregate.mean, aggregate.count)\n\nfor failure in report.failures:\n    print(failure.sample_id, failure.message)\n</code></pre>"},{"location":"guides/interoperability/","title":"Interoperability Guide","text":"<p>This guide explains how to export Themis results and load them into common tools.</p>"},{"location":"guides/interoperability/#export-formats","title":"Export formats","text":"<p>Themis exports are created via: - <code>themis.experiment.export.export_report_json</code> - <code>themis.experiment.export.export_report_csv</code> - <code>themis.experiment.export.export_html_report</code></p>"},{"location":"guides/interoperability/#json-report-schema-export_report_json","title":"JSON report schema (export_report_json)","text":"<p>Top-level keys: - <code>title</code>: Report title string. - <code>summary</code>: Run metadata and counts (includes <code>run_failures</code> and <code>evaluation_failures</code>). - <code>metrics</code>: List of aggregate metrics with <code>name</code>, <code>count</code>, and <code>mean</code>. - <code>samples</code>: Per-sample results:   - <code>sample_id</code>: Dataset id (string).   - <code>metadata</code>: Flattened metadata from generation records.   - <code>scores</code>: List of <code>{metric, value, details, metadata}</code> objects.   - <code>failures</code>: List of evaluation failure messages. - <code>rendered_sample_limit</code>: Sample limit used for the report. - <code>total_samples</code>: Total evaluation records. - <code>charts</code>: Optional chart data if provided. - <code>run_failures</code>: Generation failures (sample id + message). - <code>evaluation_failures</code>: Evaluation failures (sample id + message). - <code>metrics_rendered</code>: List of metric names rendered.</p> <p>Minimal example: <pre><code>{\n  \"title\": \"Experiment report\",\n  \"summary\": {\n    \"run_id\": \"gsm8k-gpt4-2024-01-01\",\n    \"total_samples\": 100,\n    \"run_failures\": 0,\n    \"evaluation_failures\": 0\n  },\n  \"metrics\": [\n    {\"name\": \"exact_match\", \"count\": 100, \"mean\": 0.82}\n  ],\n  \"samples\": [\n    {\n      \"sample_id\": \"gsm8k-00001\",\n      \"metadata\": {\"prompt_template\": \"gsm8k-zero-shot\"},\n      \"scores\": [{\"metric\": \"exact_match\", \"value\": 1.0, \"details\": {}, \"metadata\": {}}],\n      \"failures\": []\n    }\n  ]\n}\n</code></pre></p>"},{"location":"guides/interoperability/#csv-schema-export_report_csv","title":"CSV schema (export_report_csv)","text":"<p>CSV columns: - <code>sample_id</code> - One column per metadata field found in the run. - One column per metric: <code>metric:{metric_name}</code> - <code>failures</code> (optional)</p> <p>Example header: <pre><code>sample_id,subject,metric:exact_match,failures\n</code></pre></p>"},{"location":"guides/interoperability/#html-report-export_html_report","title":"HTML report (export_html_report)","text":"<p>The HTML report is a rendered summary designed for human review (no strict schema). It includes aggregated metrics, sample rows, and optional charts.</p>"},{"location":"guides/interoperability/#mappings-to-common-tools","title":"Mappings to common tools","text":""},{"location":"guides/interoperability/#hugging-face-datasets","title":"Hugging Face Datasets","text":"<p>Use the JSON report to build a Dataset of per-sample metrics. Requires <code>datasets</code>: <pre><code>pip install datasets\n</code></pre></p> <pre><code>import json\nfrom datasets import Dataset\n\nwith open(\"report.json\", \"r\", encoding=\"utf-8\") as handle:\n    payload = json.load(handle)\n\nrows = []\nfor sample in payload[\"samples\"]:\n    row = {\n        \"sample_id\": sample[\"sample_id\"],\n        **sample.get(\"metadata\", {}),\n    }\n    for score in sample.get(\"scores\", []):\n        row[f\"metric:{score['metric']}\"] = score[\"value\"]\n    rows.append(row)\n\ndataset = Dataset.from_list(rows)\n</code></pre> <p>If you want to upload to the Hub, see <code>themis/integrations/huggingface.py</code>.</p>"},{"location":"guides/interoperability/#weights-biases-wb","title":"Weights &amp; Biases (W&amp;B)","text":"<p>Requires <code>wandb</code>: <pre><code>pip install wandb\n</code></pre></p> <pre><code>import json\nimport wandb\n\nwandb.init(project=\"themis-eval\")\n\nwith open(\"report.json\", \"r\", encoding=\"utf-8\") as handle:\n    payload = json.load(handle)\n\nsummary = {m[\"name\"]: m[\"mean\"] for m in payload.get(\"metrics\", [])}\nwandb.summary.update(summary)\n\ntable = wandb.Table(columns=[\"sample_id\", \"metric\", \"value\"])\nfor sample in payload.get(\"samples\", []):\n    for score in sample.get(\"scores\", []):\n        table.add_data(sample[\"sample_id\"], score[\"metric\"], score[\"value\"])\n\nwandb.log({\"samples\": table})\n</code></pre> <p>For a built-in integration, see <code>themis/integrations/wandb.py</code>.</p>"},{"location":"guides/interoperability/#mlflow","title":"MLflow","text":"<p>Requires <code>mlflow</code>: <pre><code>pip install mlflow\n</code></pre></p> <pre><code>import json\nimport mlflow\n\nwith open(\"report.json\", \"r\", encoding=\"utf-8\") as handle:\n    payload = json.load(handle)\n\nwith mlflow.start_run(run_name=payload.get(\"summary\", {}).get(\"run_id\")):\n    for metric in payload.get(\"metrics\", []):\n        mlflow.log_metric(metric[\"name\"], metric[\"mean\"])\n\n    mlflow.log_artifact(\"report.json\")\n    mlflow.log_artifact(\"report.csv\")\n    mlflow.log_artifact(\"report.html\")\n</code></pre>"},{"location":"guides/interoperability/#cli-export-reminder","title":"CLI export reminder","text":"<p>The CLI supports exports via <code>--output</code>: <pre><code>themis eval gsm8k --model gpt-4 --limit 100 --output results.json\nthemis eval gsm8k --model gpt-4 --limit 100 --output results.csv\nthemis eval gsm8k --model gpt-4 --limit 100 --output results.html\n</code></pre></p>"},{"location":"guides/providers/","title":"Providers Guide","text":"<p>Themis routes generation via provider + model.</p>"},{"location":"guides/providers/#model-string-formats","title":"Model String Formats","text":"<ul> <li>Auto-detected model name: <code>\"gpt-4\"</code>, <code>\"claude-3-opus-20240229\"</code></li> <li>Canonical key: <code>\"provider:model_id\"</code> (recommended for specs)</li> </ul> <p>Examples:</p> <pre><code># Auto-detected provider (litellm)\nevaluate(\"gsm8k\", model=\"gpt-4\")\n\n# Explicit provider:model_id for ExperimentSpec\nspec = ExperimentSpec(..., model=\"litellm:gpt-4\", ...)\n</code></pre>"},{"location":"guides/providers/#fake-provider","title":"Fake Provider","text":"<p>Use fake models for local smoke tests without credentials:</p> <pre><code>evaluate(\"demo\", model=\"fake-math-llm\", limit=5)\n# or\nspec = ExperimentSpec(..., model=\"fake:fake-math-llm\", ...)\n</code></pre>"},{"location":"guides/providers/#credentials","title":"Credentials","text":"<p>Set API keys only for real providers:</p> <pre><code>export OPENAI_API_KEY=\"...\"\nexport ANTHROPIC_API_KEY=\"...\"\n</code></pre>"},{"location":"guides/storage/","title":"Storage Guide","text":"<p>Themis stores run artifacts under a storage root (default: <code>.cache/experiments</code>).</p>"},{"location":"guides/storage/#configure-storage","title":"Configure Storage","text":""},{"location":"guides/storage/#python-api","title":"Python API","text":"<pre><code>report = evaluate(\n    \"gsm8k\",\n    model=\"gpt-4\",\n    storage=\".cache/experiments\",\n    run_id=\"gsm8k-gpt4\",\n    resume=True,\n)\n</code></pre>"},{"location":"guides/storage/#spec-api","title":"Spec API","text":"<pre><code>report = ExperimentSession().run(\n    spec,\n    storage=StorageSpec(path=\".cache/experiments\", cache=True),\n)\n</code></pre>"},{"location":"guides/storage/#cli","title":"CLI","text":"<pre><code>themis eval gsm8k --model gpt-4 --storage .cache/experiments\n</code></pre> <p>You can also set a default path:</p> <pre><code>export THEMIS_STORAGE=\"~/.themis/experiments\"\n</code></pre>"},{"location":"guides/storage/#run-management","title":"Run Management","text":"<pre><code># list runs\nthemis list runs --verbose\n\n# preview cleanup\nthemis clean --older-than 30 --dry-run\n\n# delete old runs\nthemis clean --older-than 30\n</code></pre>"},{"location":"guides/storage/#stored-artifacts","title":"Stored Artifacts","text":"<p>Typical per-run artifacts include generation/evaluation caches and exported reports. Use <code>themis share &lt;RUN_ID&gt;</code> to create shareable badge + markdown outputs.</p>"},{"location":"reference/api-server/","title":"Themis API Server","text":"<p>The Themis API server provides REST and WebSocket endpoints for accessing experiment results, comparing runs, and monitoring experiments in real-time.</p>"},{"location":"reference/api-server/#installation","title":"Installation","text":"<p>The API server requires optional dependencies:</p> <pre><code>pip install themis[server]\n# or\nuv pip install themis[server]\n</code></pre>"},{"location":"reference/api-server/#quick-start","title":"Quick Start","text":""},{"location":"reference/api-server/#start-the-server","title":"Start the Server","text":"<pre><code># Default (port 8080, localhost)\nthemis serve\n\n# Custom port and storage\nthemis serve --port 3000 --storage ~/.themis/runs\n\n# Development mode with auto-reload\nthemis serve --reload --host 0.0.0.0\n</code></pre>"},{"location":"reference/api-server/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from themis.server import create_app\nimport uvicorn\n\n# Create app\napp = create_app(storage_path=\".cache/experiments\")\n\n# Run server\nuvicorn.run(app, host=\"0.0.0.0\", port=8080)\n</code></pre>"},{"location":"reference/api-server/#api-endpoints","title":"API Endpoints","text":""},{"location":"reference/api-server/#health-check","title":"Health Check","text":"<p>GET <code>/</code></p> <p>Check if the server is running.</p> <pre><code>curl http://localhost:8080/\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"ok\",\n  \"service\": \"themis-api\",\n  \"version\": \"1.0.0\"\n}\n</code></pre></p>"},{"location":"reference/api-server/#list-runs","title":"List Runs","text":"<p>GET <code>/api/runs</code></p> <p>List all experiment runs with summary metrics.</p> <pre><code>curl http://localhost:8080/api/runs\n</code></pre> <p>Response: <pre><code>[\n  {\n    \"run_id\": \"run-2024-01-15\",\n    \"experiment_id\": \"default\",\n    \"status\": \"completed\",\n    \"num_samples\": 100,\n    \"metrics\": {\n      \"ExactMatch\": 0.85,\n      \"BLEU\": 0.72\n    },\n    \"created_at\": null\n  }\n]\n</code></pre></p>"},{"location":"reference/api-server/#get-run-details","title":"Get Run Details","text":"<p>GET <code>/api/runs/{run_id}</code></p> <p>Get detailed information about a specific run, including all samples.</p> <pre><code>curl http://localhost:8080/api/runs/my-run-id\n</code></pre> <p>Response: <pre><code>{\n  \"run_id\": \"my-run-id\",\n  \"experiment_id\": \"default\",\n  \"status\": \"completed\",\n  \"num_samples\": 100,\n  \"metrics\": {\n    \"ExactMatch\": 0.85\n  },\n  \"samples\": [\n    {\n      \"id\": \"sample-1\",\n      \"prompt\": \"What is 2+2?\",\n      \"response\": \"4\",\n      \"scores\": {\n        \"ExactMatch\": 1.0\n      }\n    }\n  ],\n  \"metadata\": {}\n}\n</code></pre></p>"},{"location":"reference/api-server/#delete-run","title":"Delete Run","text":"<p>DELETE <code>/api/runs/{run_id}</code></p> <p>Delete a run. This endpoint currently returns <code>501 Not Implemented</code> because the storage adapter does not expose a public delete API yet.</p> <pre><code>curl -X DELETE http://localhost:8080/api/runs/my-run-id\n</code></pre>"},{"location":"reference/api-server/#compare-runs","title":"Compare Runs","text":"<p>POST <code>/api/compare</code></p> <p>Compare multiple runs with statistical significance testing.</p> <p>Request body: <pre><code>{\n  \"run_ids\": [\"run-1\", \"run-2\"],\n  \"metrics\": [\"ExactMatch\", \"BLEU\"],\n  \"statistical_test\": \"bootstrap\",\n  \"alpha\": 0.05\n}\n</code></pre></p> <pre><code>curl -X POST http://localhost:8080/api/compare \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"run_ids\": [\"run-1\", \"run-2\"], \"statistical_test\": \"bootstrap\"}'\n</code></pre> <p>Response: <pre><code>{\n  \"run_ids\": [\"run-1\", \"run-2\"],\n  \"metrics\": [\"ExactMatch\"],\n  \"best_run_per_metric\": {\n    \"ExactMatch\": \"run-1\"\n  },\n  \"overall_best_run\": \"run-1\",\n  \"pairwise_results\": [\n    {\n      \"metric\": \"ExactMatch\",\n      \"run_a\": \"run-1\",\n      \"run_b\": \"run-2\",\n      \"run_a_mean\": 0.85,\n      \"run_b_mean\": 0.80,\n      \"delta\": 0.05,\n      \"delta_percent\": 6.25,\n      \"winner\": \"run-1\",\n      \"significant\": true,\n      \"p_value\": 0.001\n    }\n  ]\n}\n</code></pre></p>"},{"location":"reference/api-server/#list-benchmarks","title":"List Benchmarks","text":"<p>GET <code>/api/benchmarks</code></p> <p>List available benchmark presets.</p> <pre><code>curl http://localhost:8080/api/benchmarks\n</code></pre> <p>Response: <pre><code>{\n  \"benchmarks\": [\n    \"demo\",\n    \"math500\",\n    \"gsm8k\",\n    \"gsm-symbolic\",\n    \"aime24\",\n    \"aime25\",\n    \"mmlu-pro\",\n    \"supergpqa\",\n    \"gpqa\",\n    \"medmcqa\",\n    \"commonsense_qa\",\n    \"coqa\"\n  ]\n}\n</code></pre></p>"},{"location":"reference/api-server/#websocket-api","title":"WebSocket API","text":""},{"location":"reference/api-server/#connection","title":"Connection","text":"<p>Connect to the WebSocket endpoint:</p> <pre><code>ws://localhost:8080/ws\n</code></pre>"},{"location":"reference/api-server/#messages-from-client","title":"Messages from Client","text":""},{"location":"reference/api-server/#ping","title":"Ping","text":"<pre><code>{\"type\": \"ping\"}\n</code></pre> <p>Response: <pre><code>{\"type\": \"pong\"}\n</code></pre></p>"},{"location":"reference/api-server/#subscribe-to-run","title":"Subscribe to Run","text":"<pre><code>{\n  \"type\": \"subscribe\",\n  \"run_id\": \"my-run-id\"\n}\n</code></pre> <p>Response: <pre><code>{\n  \"type\": \"subscribed\",\n  \"run_id\": \"my-run-id\"\n}\n</code></pre></p>"},{"location":"reference/api-server/#unsubscribe","title":"Unsubscribe","text":"<pre><code>{\n  \"type\": \"unsubscribe\",\n  \"run_id\": \"my-run-id\"\n}\n</code></pre>"},{"location":"reference/api-server/#messages-from-server","title":"Messages from Server","text":""},{"location":"reference/api-server/#run-started","title":"Run Started","text":"<pre><code>{\n  \"type\": \"run_started\",\n  \"run_id\": \"my-run-id\",\n  \"data\": {...}\n}\n</code></pre>"},{"location":"reference/api-server/#run-progress","title":"Run Progress","text":"<pre><code>{\n  \"type\": \"run_progress\",\n  \"run_id\": \"my-run-id\",\n  \"progress\": 0.5\n}\n</code></pre>"},{"location":"reference/api-server/#run-completed","title":"Run Completed","text":"<pre><code>{\n  \"type\": \"run_completed\",\n  \"run_id\": \"my-run-id\",\n  \"data\": {...}\n}\n</code></pre>"},{"location":"reference/api-server/#error","title":"Error","text":"<pre><code>{\n  \"type\": \"error\",\n  \"message\": \"Error description\"\n}\n</code></pre>"},{"location":"reference/api-server/#python-client-examples","title":"Python Client Examples","text":""},{"location":"reference/api-server/#rest-api-client","title":"REST API Client","text":"<pre><code>import requests\n\nbase_url = \"http://localhost:8080\"\n\n# List runs\nresponse = requests.get(f\"{base_url}/api/runs\")\nruns = response.json()\n\n# Get run details\nrun_id = runs[0]['run_id']\nresponse = requests.get(f\"{base_url}/api/runs/{run_id}\")\ndetail = response.json()\n\n# Compare runs\nresponse = requests.post(\n    f\"{base_url}/api/compare\",\n    json={\n        \"run_ids\": [\"run-1\", \"run-2\"],\n        \"statistical_test\": \"bootstrap\",\n        \"alpha\": 0.05\n    }\n)\ncomparison = response.json()\n</code></pre>"},{"location":"reference/api-server/#websocket-client","title":"WebSocket Client","text":"<pre><code>import asyncio\nimport json\nimport websockets\n\nasync def monitor_runs():\n    uri = \"ws://localhost:8080/ws\"\n\n    async with websockets.connect(uri) as websocket:\n        # Subscribe to run\n        await websocket.send(json.dumps({\n            \"type\": \"subscribe\",\n            \"run_id\": \"my-run\"\n        }))\n\n        # Listen for updates\n        async for message in websocket:\n            data = json.loads(message)\n            print(f\"Update: {data}\")\n\nasyncio.run(monitor_runs())\n</code></pre>"},{"location":"reference/api-server/#interactive-documentation","title":"Interactive Documentation","text":"<p>FastAPI automatically generates interactive API documentation:</p> <ul> <li>Swagger UI: <code>http://localhost:8080/docs</code></li> <li>ReDoc: <code>http://localhost:8080/redoc</code></li> </ul> <p>These interfaces allow you to: - Browse all available endpoints - See request/response schemas - Try out API calls directly from the browser</p>"},{"location":"reference/api-server/#cors-configuration","title":"CORS Configuration","text":"<p>By default, the server allows requests from any origin (CORS <code>*</code>). For production, configure appropriate origins:</p> <pre><code>from themis.server.app import create_app\n\napp = create_app(storage_path=\".cache/experiments\")\n\n# Update CORS settings\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"https://yourdomain.com\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"reference/api-server/#authentication","title":"Authentication","text":"<p>The default server has no authentication. For production use, consider adding:</p> <ul> <li>API key authentication</li> <li>JWT tokens</li> <li>OAuth2 integration</li> </ul> <p>Example with API key middleware:</p> <pre><code>from fastapi import Security, HTTPException\nfrom fastapi.security import APIKeyHeader\n\nAPI_KEY = \"your-secret-key\"\napi_key_header = APIKeyHeader(name=\"X-API-Key\")\n\ndef get_api_key(api_key: str = Security(api_key_header)):\n    if api_key != API_KEY:\n        raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n    return api_key\n\n# Use in endpoints\n@app.get(\"/api/runs\", dependencies=[Depends(get_api_key)])\nasync def list_runs():\n    ...\n</code></pre>"},{"location":"reference/api-server/#deployment","title":"Deployment","text":""},{"location":"reference/api-server/#docker","title":"Docker","text":"<pre><code>FROM python:3.12-slim\n\nWORKDIR /app\n\n# Install Themis with server extras\nRUN pip install themis[server]\n\n# Expose port\nEXPOSE 8080\n\n# Run server\nCMD [\"themis\", \"serve\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n</code></pre>"},{"location":"reference/api-server/#systemd-service","title":"systemd Service","text":"<pre><code>[Unit]\nDescription=Themis API Server\nAfter=network.target\n\n[Service]\nType=simple\nUser=themis\nWorkingDirectory=/home/themis\nExecStart=/usr/local/bin/themis serve --host 0.0.0.0 --port 8080\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"reference/api-server/#nginx-reverse-proxy","title":"Nginx Reverse Proxy","text":"<pre><code>server {\n    listen 80;\n    server_name themis.example.com;\n\n    location / {\n        proxy_pass http://localhost:8080;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n\n    # WebSocket support\n    location /ws {\n        proxy_pass http://localhost:8080/ws;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre>"},{"location":"reference/api-server/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/api-server/#port-already-in-use","title":"Port Already in Use","text":"<pre><code># Check what's using the port\nlsof -i :8080\n\n# Use a different port\nthemis serve --port 8081\n</code></pre>"},{"location":"reference/api-server/#import-errors","title":"Import Errors","text":"<p>If you get <code>ImportError: cannot import name 'create_app'</code>:</p> <pre><code># Install server dependencies\npip install themis[server]\n</code></pre>"},{"location":"reference/api-server/#cors-issues","title":"CORS Issues","text":"<p>If requests from your web app are blocked:</p> <ol> <li>Check browser console for CORS errors</li> <li>Verify <code>allow_origins</code> in CORS middleware</li> <li>Ensure you're using the correct protocol (http/https)</li> </ol>"},{"location":"reference/api-server/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Use gunicorn/uvicorn workers: For production, run with multiple workers    <pre><code>gunicorn themis.server.app:app -w 4 -k uvicorn.workers.UvicornWorker\n</code></pre></p> </li> <li> <p>Enable HTTP/2: Use a reverse proxy (nginx, caddy) with HTTP/2 support</p> </li> <li> <p>Add caching: Cache frequent queries (list runs, etc.)</p> </li> <li> <p>Limit response sizes: Paginate large result sets</p> </li> <li> <p>Use connection pooling: For database backends</p> </li> </ol>"},{"location":"reference/api-server/#next-steps","title":"Next Steps","text":"<ul> <li>Build a web dashboard using the API (HTML/JavaScript or React)</li> <li>Implement real-time run monitoring via WebSocket</li> <li>Add authentication and authorization</li> <li>Deploy to production with proper security</li> </ul>"},{"location":"reference/benchmarks/","title":"Benchmarks Reference","text":"<p>Complete reference for all built-in benchmarks in Themis.</p>"},{"location":"reference/benchmarks/#overview","title":"Overview","text":"<p>Themis includes 19 built-in benchmarks covering: - Math Reasoning: GSM8K, MATH500, GSM-Symbolic, AIME24, AIME25, AMC23, OlympiadBench, BeyondAIME - Knowledge &amp; Science: MMLU-Pro, SuperGPQA, GPQA, SciQ - Medicine: MedMCQA, MedQA - Commonsense: CommonsenseQA, PIQA, Social IQA - Conversational QA: CoQA - Quick Testing: Demo</p>"},{"location":"reference/benchmarks/#benchmark-catalog-summary","title":"Benchmark Catalog (Summary)","text":"Benchmark Domain Format Notes demo Quick testing Short QA Built-in tiny dataset gsm8k Math Free-form Grade school math word problems math500 Math Free-form Competition math (MATH) gsm-symbolic Math Free-form Symbolic math variations aime24 Math Free-form AIME 2024 aime25 Math Free-form AIME 2025 amc23 Math Free-form AMC 2023 olympiadbench Math Free-form Olympiad-style problems beyondaime Math Free-form Advanced contest problems mmlu-pro Knowledge MCQ (letter) Professional-level subjects supergpqa Science MCQ (letter) Graduate-level science gpqa Science MCQ (letter) Graduate-level QA sciq Science MCQ (letter) Science questions medmcqa Medicine MCQ (letter) Medical exams med_qa Medicine MCQ (letter) Medical QA commonsense_qa Commonsense MCQ (letter) Commonsense reasoning piqa Commonsense MCQ (letter) Physical commonsense social_i_qa Commonsense MCQ (letter) Social reasoning coqa Conversational Free-form Multi-turn QA"},{"location":"reference/benchmarks/#math-benchmarks","title":"Math Benchmarks","text":""},{"location":"reference/benchmarks/#demo","title":"demo","text":"<p>Quick testing benchmark with minimal samples</p> <ul> <li>Dataset Size: 10 samples</li> <li>Domain: Elementary arithmetic</li> <li>Difficulty: Easy</li> <li>Source: Subset of GSM8K</li> <li>Metrics: ExactMatch, MathVerifyAccuracy</li> <li>License: MIT</li> </ul> <p>Usage: <pre><code>from themis import evaluate\n\n# Test without API key\nresult = evaluate(\"demo\", model=\"fake-math-llm\")\n\n# Test with real model\nresult = evaluate(\"demo\", model=\"gpt-4\")\n</code></pre></p> <p>Example Problems: - \"Janet's ducks lay 16 eggs per day. She eats three for breakfast...\" - \"A robe takes 2 bolts of blue fiber and half that much white fiber...\"</p> <p>When to use: - Testing your setup - Verifying Themis installation - Quick experiments without API costs</p>"},{"location":"reference/benchmarks/#gsm8k","title":"gsm8k","text":"<p>Grade School Math 8K - Elementary math word problems</p> <ul> <li>Dataset Size: 8,500+ problems</li> <li>Domain: Grade school mathematics (K-8)</li> <li>Difficulty: Elementary to middle school</li> <li>Source: GSM8K Paper</li> <li>Metrics: ExactMatch, MathVerifyAccuracy</li> <li>License: MIT</li> </ul> <p>Usage: <pre><code>result = evaluate(\"gsm8k\", model=\"gpt-4\", limit=100)\n</code></pre></p> <p>Example Problems: - Multi-step arithmetic word problems - Real-world scenarios (money, measurements, time) - Requires basic arithmetic and reasoning</p> <p>Prompt Template: <pre><code>Problem: {prompt}\nSolution:\n</code></pre></p> <p>Evaluation: - Extracts final numerical answer - Compares with reference using exact match and math verification</p> <p>Typical Scores: - GPT-4: ~90-95% - GPT-3.5: ~55-60% - Claude-3: ~90-95%</p>"},{"location":"reference/benchmarks/#math500","title":"math500","text":"<p>MATH500 - Advanced competition-level math</p> <ul> <li>Dataset Size: 500 problems</li> <li>Domain: High school to competition math</li> <li>Difficulty: Advanced (AIME, AMC, IMO level)</li> <li>Source: MATH Dataset</li> <li>Metrics: ExactMatch, MathVerifyAccuracy</li> <li>License: MIT</li> </ul> <p>Usage: <pre><code>result = evaluate(\"math500\", model=\"gpt-4\")\n</code></pre></p> <p>Example Problems: - Algebra, geometry, calculus, number theory - Requires multiple steps and deep reasoning - Often includes LaTeX formatting</p> <p>Prompt Template: <pre><code>Solve this problem: {problem}\n</code></pre></p> <p>Typical Scores: - GPT-4: ~30-40% - GPT-3.5: ~5-10% - Claude-3: ~35-45%</p>"},{"location":"reference/benchmarks/#aime24","title":"aime24","text":"<p>AIME 2024 - American Invitational Mathematics Examination</p> <ul> <li>Dataset Size: 30 problems</li> <li>Domain: Competition mathematics</li> <li>Difficulty: Very challenging (top high school level)</li> <li>Source: 2024 AIME exam</li> <li>Metrics: ExactMatch, MathVerifyAccuracy</li> <li>License: Public domain (exam questions)</li> </ul> <p>Usage: <pre><code>result = evaluate(\"aime24\", model=\"gpt-4\")\n</code></pre></p> <p>Example Problems: - Advanced algebra, geometry, combinatorics - Integer answers from 000 to 999 - Requires deep mathematical reasoning</p> <p>Prompt Template: <pre><code>Problem: {problem}\n\nSolution:\n</code></pre></p> <p>Typical Scores: - GPT-4: ~10-15% (3-5 out of 30) - GPT-3.5: ~0-5% - Claude-3: ~10-20%</p> <p>Note: This is an extremely challenging benchmark. Even top models struggle.</p>"},{"location":"reference/benchmarks/#knowledge-benchmarks","title":"Knowledge Benchmarks","text":""},{"location":"reference/benchmarks/#mmlu-pro","title":"mmlu-pro","text":"<p>MMLU-Pro - Massive Multitask Language Understanding (Professional)</p> <ul> <li>Dataset Size: Thousands of questions</li> <li>Domain: Multiple subjects (STEM, humanities, social sciences)</li> <li>Difficulty: College and professional level</li> <li>Source: MMLU-Pro</li> <li>Metrics: ExactMatch</li> <li>License: MIT</li> </ul> <p>Usage: <pre><code>result = evaluate(\"mmlu-pro\", model=\"gpt-4\", limit=1000)\n</code></pre></p> <p>Subjects: - Physics, Chemistry, Biology - History, Law, Economics - Computer Science, Mathematics - And many more</p> <p>Format: Multiple choice questions</p> <p>Typical Scores: - GPT-4: ~70-75% - GPT-3.5: ~45-50% - Claude-3: ~70-75%</p>"},{"location":"reference/benchmarks/#supergpqa","title":"supergpqa","text":"<p>SuperGPQA - Advanced reasoning and knowledge</p> <ul> <li>Dataset Size: Varies</li> <li>Domain: Expert-level questions</li> <li>Difficulty: Graduate level and beyond</li> <li>Source: GPQA</li> <li>Metrics: ExactMatch</li> <li>License: CC-BY</li> </ul> <p>Usage: <pre><code>result = evaluate(\"supergpqa\", model=\"gpt-4\")\n</code></pre></p> <p>Characteristics: - Requires expert knowledge - Multi-step reasoning - Often requires combining multiple concepts</p> <p>Typical Scores: - GPT-4: ~35-40% - GPT-3.5: ~20-25% - Claude-3: ~40-45%</p>"},{"location":"reference/benchmarks/#science-medical-benchmarks","title":"Science &amp; Medical Benchmarks","text":""},{"location":"reference/benchmarks/#gpqa","title":"gpqa","text":"<p>GPQA - Graduate-level science questions</p> <ul> <li>Domain: Science</li> <li>Format: Multiple choice (letter)</li> <li>Metrics: ExactMatch</li> </ul> <p>Usage: <pre><code>result = evaluate(\"gpqa\", model=\"gpt-4\")\n</code></pre></p>"},{"location":"reference/benchmarks/#sciq","title":"sciq","text":"<p>SciQ - Science question answering</p> <ul> <li>Domain: Science</li> <li>Format: Multiple choice (letter)</li> <li>Metrics: ExactMatch</li> </ul> <p>Usage: <pre><code>result = evaluate(\"sciq\", model=\"gpt-4\", limit=100)\n</code></pre></p>"},{"location":"reference/benchmarks/#medmcqa","title":"medmcqa","text":"<p>MedMCQA - Medical entrance exam questions</p> <ul> <li>Domain: Medicine</li> <li>Format: Multiple choice (letter)</li> <li>Metrics: ExactMatch</li> </ul> <p>Usage: <pre><code>result = evaluate(\"medmcqa\", model=\"gpt-4\", limit=200)\n</code></pre></p>"},{"location":"reference/benchmarks/#med_qa","title":"med_qa","text":"<p>MedQA - Medical QA benchmark</p> <ul> <li>Domain: Medicine</li> <li>Format: Multiple choice (letter)</li> <li>Metrics: ExactMatch</li> </ul> <p>Usage: <pre><code>result = evaluate(\"med_qa\", model=\"gpt-4\", limit=200)\n</code></pre></p>"},{"location":"reference/benchmarks/#commonsense-benchmarks","title":"Commonsense Benchmarks","text":""},{"location":"reference/benchmarks/#commonsense_qa","title":"commonsense_qa","text":"<p>CommonsenseQA - Commonsense reasoning</p> <ul> <li>Domain: Commonsense</li> <li>Format: Multiple choice (letter)</li> <li>Metrics: ExactMatch</li> <li>Note: Uses the validation split because test labels are not public.</li> </ul> <p>Usage: <pre><code>result = evaluate(\"commonsense_qa\", model=\"gpt-4\", limit=200)\n</code></pre></p>"},{"location":"reference/benchmarks/#piqa","title":"piqa","text":"<p>PIQA - Physical commonsense reasoning</p> <ul> <li>Domain: Commonsense</li> <li>Format: Multiple choice (letter)</li> <li>Metrics: ExactMatch</li> <li>Note: Uses the validation split because test labels are not public.</li> </ul> <p>Usage: <pre><code>result = evaluate(\"piqa\", model=\"gpt-4\", limit=200)\n</code></pre></p>"},{"location":"reference/benchmarks/#social_i_qa","title":"social_i_qa","text":"<p>Social IQA - Social reasoning</p> <ul> <li>Domain: Commonsense</li> <li>Format: Multiple choice (letter)</li> <li>Metrics: ExactMatch</li> <li>Note: Uses the validation split because test labels are not public.</li> </ul> <p>Usage: <pre><code>result = evaluate(\"social_i_qa\", model=\"gpt-4\", limit=200)\n</code></pre></p>"},{"location":"reference/benchmarks/#conversational-qa-benchmarks","title":"Conversational QA Benchmarks","text":""},{"location":"reference/benchmarks/#coqa","title":"coqa","text":"<p>CoQA - Conversational question answering</p> <ul> <li>Domain: Conversational QA</li> <li>Format: Free-form</li> <li>Metrics: ExactMatch</li> <li>Note: Uses the validation split because test labels are not public.</li> </ul> <p>Usage: <pre><code>result = evaluate(\"coqa\", model=\"gpt-4\", limit=200)\n</code></pre></p>"},{"location":"reference/benchmarks/#additional-math-benchmarks","title":"Additional Math Benchmarks","text":""},{"location":"reference/benchmarks/#gsm-symbolic","title":"gsm-symbolic","text":"<p>GSM-Symbolic - Symbolic variants of GSM8K</p> <ul> <li>Domain: Math</li> <li>Format: Free-form</li> <li>Metrics: MathVerifyAccuracy</li> </ul> <p>Usage: <pre><code>result = evaluate(\"gsm-symbolic\", model=\"gpt-4\", limit=200)\n</code></pre></p>"},{"location":"reference/benchmarks/#aime25","title":"aime25","text":"<p>AIME 2025 - Competition math</p> <ul> <li>Domain: Math</li> <li>Format: Free-form</li> <li>Metrics: MathVerifyAccuracy</li> </ul> <p>Usage: <pre><code>result = evaluate(\"aime25\", model=\"gpt-4\")\n</code></pre></p>"},{"location":"reference/benchmarks/#amc23","title":"amc23","text":"<p>AMC 2023 - Competition math</p> <ul> <li>Domain: Math</li> <li>Format: Free-form</li> <li>Metrics: MathVerifyAccuracy</li> </ul> <p>Usage: <pre><code>result = evaluate(\"amc23\", model=\"gpt-4\")\n</code></pre></p>"},{"location":"reference/benchmarks/#olympiadbench","title":"olympiadbench","text":"<p>OlympiadBench - Olympiad-style problems</p> <ul> <li>Domain: Math</li> <li>Format: Free-form</li> <li>Metrics: MathVerifyAccuracy</li> </ul> <p>Usage: <pre><code>result = evaluate(\"olympiadbench\", model=\"gpt-4\")\n</code></pre></p>"},{"location":"reference/benchmarks/#beyondaime","title":"beyondaime","text":"<p>BeyondAIME - Advanced contest math</p> <ul> <li>Domain: Math</li> <li>Format: Free-form</li> <li>Metrics: MathVerifyAccuracy</li> </ul> <p>Usage: <pre><code>result = evaluate(\"beyondaime\", model=\"gpt-4\")\n</code></pre></p>"},{"location":"reference/benchmarks/#benchmark-comparison-core-set","title":"Benchmark Comparison (Core Set)","text":"Benchmark Size Difficulty Domain Typical Time demo 10 Easy Math &lt; 1 min gsm8k 8,500 Medium Math 30-60 min math500 500 Hard Math 5-10 min aime24 30 Very Hard Math 1-2 min mmlu-pro Large Medium-Hard Knowledge 20-40 min supergpqa Medium Very Hard Knowledge 10-20 min <p>Times are estimates with GPT-4 and 8 workers</p>"},{"location":"reference/benchmarks/#coverage-gaps-by-domain","title":"Coverage Gaps by Domain","text":"<ul> <li>Multimodal: No built-in image/audio/video benchmarks.</li> <li>Tool-use / agentic: No built-in tool-use or multi-step tool benchmarks.</li> <li>Long-context: No dedicated long-context datasets (100K+ tokens).</li> <li>Safety/alignment: No built-in safety or red-teaming benchmarks.</li> </ul> <p>If these domains are critical, integrate external benchmarks via custom datasets.</p>"},{"location":"reference/benchmarks/#usage-recommendations","title":"Usage Recommendations","text":""},{"location":"reference/benchmarks/#for-quick-testing","title":"For Quick Testing","text":"<p>\u2192 Use <code>demo</code> (10 samples, no API key needed)</p>"},{"location":"reference/benchmarks/#for-math-evaluation","title":"For Math Evaluation","text":"<p>\u2192 Start with <code>gsm8k</code> (most widely used) \u2192 Then try <code>math500</code> for harder problems \u2192 Use <code>aime24</code>/<code>aime25</code>/<code>amc23</code>/<code>olympiadbench</code> for competition difficulty \u2192 Use <code>gsm-symbolic</code> to test symbolic variants</p>"},{"location":"reference/benchmarks/#for-general-knowledge","title":"For General Knowledge","text":"<p>\u2192 Use <code>mmlu-pro</code> for broad coverage \u2192 Use <code>supergpqa</code> for expert-level questions</p>"},{"location":"reference/benchmarks/#for-science-medical","title":"For Science &amp; Medical","text":"<p>\u2192 Use <code>gpqa</code> or <code>sciq</code> for science MCQ \u2192 Use <code>medmcqa</code> or <code>med_qa</code> for medical QA</p>"},{"location":"reference/benchmarks/#for-commonsense-reasoning","title":"For Commonsense Reasoning","text":"<p>\u2192 Use <code>commonsense_qa</code>, <code>piqa</code>, and <code>social_i_qa</code> for diverse commonsense tasks</p>"},{"location":"reference/benchmarks/#for-conversational-qa","title":"For Conversational QA","text":"<p>\u2192 Use <code>coqa</code> for multi-turn question answering</p>"},{"location":"reference/benchmarks/#for-research-papers","title":"For Research Papers","text":"<p>\u2192 Use <code>gsm8k</code> and <code>math500</code> (widely reported) \u2192 Include multiple benchmarks for comprehensive evaluation</p>"},{"location":"reference/benchmarks/#adding-custom-benchmarks","title":"Adding Custom Benchmarks","text":"<p>You can register your own benchmarks:</p> <pre><code>from themis.presets.benchmarks import BenchmarkPreset, register_benchmark\nfrom themis.generation.templates import PromptTemplate\nfrom themis.evaluation import extractors, metrics\n\ndef my_dataset_loader(limit=None):\n    data = load_my_data()\n    return data[:limit] if limit else data\n\npreset = BenchmarkPreset(\n    name=\"my-benchmark\",\n    prompt_template=PromptTemplate(name=\"custom\", template=\"Q: {question}\\nA:\"),\n    metrics=[metrics.ExactMatch()],\n    extractor=extractors.IdentityExtractor(),\n    dataset_loader=my_dataset_loader,\n    description=\"My custom benchmark\",\n)\n\nregister_benchmark(preset)\n\n# Use it\nresult = evaluate(\"my-benchmark\", model=\"gpt-4\")\n</code></pre>"},{"location":"reference/benchmarks/#benchmark-statistics","title":"Benchmark Statistics","text":""},{"location":"reference/benchmarks/#gsm8k_1","title":"GSM8K","text":"<ul> <li>Problems: 8,500</li> <li>Average length: 3-5 sentences</li> <li>Topics: Addition, subtraction, multiplication, division, percentages, ratios</li> <li>Format: Natural language word problems</li> <li>Evaluation: Numerical answer extraction</li> </ul>"},{"location":"reference/benchmarks/#math500_1","title":"MATH500","text":"<ul> <li>Problems: 500</li> <li>Average length: 5-10 sentences</li> <li>Topics: Algebra, geometry, calculus, number theory, combinatorics</li> <li>Format: LaTeX mathematical notation</li> <li>Evaluation: Symbolic and numerical matching</li> </ul>"},{"location":"reference/benchmarks/#aime24_1","title":"AIME24","text":"<ul> <li>Problems: 30</li> <li>Average length: 3-8 sentences</li> <li>Topics: Advanced competition mathematics</li> <li>Format: Natural language with mathematical notation</li> <li>Evaluation: Integer answers (000-999)</li> </ul>"},{"location":"reference/benchmarks/#best-practices","title":"Best Practices","text":""},{"location":"reference/benchmarks/#1-start-with-demo","title":"1. Start with Demo","text":"<p>Always test with <code>demo</code> first: <pre><code>result = evaluate(\"demo\", model=\"fake-math-llm\")\n</code></pre></p>"},{"location":"reference/benchmarks/#2-use-appropriate-limits","title":"2. Use Appropriate Limits","text":"<p>For expensive models: <pre><code># Test with small sample\nresult = evaluate(\"gsm8k\", model=\"gpt-4\", limit=100)\n\n# Full evaluation once validated\nresult = evaluate(\"gsm8k\", model=\"gpt-4\")\n</code></pre></p>"},{"location":"reference/benchmarks/#3-choose-right-benchmark","title":"3. Choose Right Benchmark","text":"<p>Match benchmark to your use case: - Testing math ability \u2192 gsm8k or math500 - Competition performance \u2192 aime24 - General knowledge \u2192 mmlu-pro - Expert reasoning \u2192 supergpqa</p>"},{"location":"reference/benchmarks/#4-report-multiple-benchmarks","title":"4. Report Multiple Benchmarks","text":"<p>For research, evaluate on multiple benchmarks: <pre><code>benchmarks = [\"gsm8k\", \"math500\", \"aime24\"]\n\nfor benchmark in benchmarks:\n    result = evaluate(benchmark,\n        model=\"gpt-4\",\n        run_id=f\"{benchmark}-gpt4\",\n    )\n</code></pre></p>"},{"location":"reference/benchmarks/#see-also","title":"See Also","text":"<ul> <li>Presets API - Benchmark API reference</li> <li>Evaluation Guide - Using benchmarks</li> <li>Custom Datasets - Create your own</li> </ul>"},{"location":"reference/session/","title":"ExperimentSession","text":"<p><code>ExperimentSession</code> is the primary entry point for running experiments using explicit specs.</p>"},{"location":"reference/session/#usage","title":"Usage","text":"<pre><code>from themis.evaluation.pipeline import MetricPipeline\nfrom themis.evaluation import extractors, metrics\nfrom themis.session import ExperimentSession\nfrom themis.specs import ExperimentSpec, ExecutionSpec, StorageSpec\n\npipeline = MetricPipeline(\n    extractor=extractors.IdentityExtractor(),\n    metrics=[metrics.ResponseLength()],\n)\n\nspec = ExperimentSpec(\n    dataset=[{\"id\": \"1\", \"question\": \"2+2\", \"answer\": \"4\"}],\n    prompt=\"Solve: {question}\",\n    model=\"fake:fake-math-llm\",\n    sampling={\"temperature\": 0.0, \"max_tokens\": 128},\n    pipeline=pipeline,\n)\n\nreport = ExperimentSession().run(\n    spec,\n    execution=ExecutionSpec(workers=2),\n    storage=StorageSpec(path=\".cache/experiments\"),\n)\n</code></pre>"},{"location":"reference/session/#notes","title":"Notes","text":"<ul> <li><code>ExperimentSession</code> requires an evaluation pipeline that implements <code>EvaluationPipelineContract</code>.</li> <li>The model string supports <code>provider:model_id</code> format.</li> </ul>"},{"location":"reference/specs/","title":"Specs","text":"<p>The API uses explicit spec objects to configure experiments.</p>"},{"location":"reference/specs/#experimentspec","title":"ExperimentSpec","text":"<pre><code>from themis.specs import ExperimentSpec\n\nspec = ExperimentSpec(\n    dataset=[{\"id\": \"1\", \"question\": \"2+2\", \"answer\": \"4\"}],\n    prompt=\"Solve: {question}\",\n    model=\"fake:fake-math-llm\",\n    sampling={\"temperature\": 0.0, \"max_tokens\": 128},\n    pipeline=my_pipeline,\n    run_id=\"run-1\",\n)\n</code></pre>"},{"location":"reference/specs/#executionspec","title":"ExecutionSpec","text":"<pre><code>from themis.specs import ExecutionSpec\n\nexecution = ExecutionSpec(\n    backend=my_backend,\n    workers=4,\n    max_retries=3,\n)\n</code></pre>"},{"location":"reference/specs/#storagespec","title":"StorageSpec","text":"<pre><code>from themis.specs import StorageSpec\n\nstorage = StorageSpec(\n    backend=my_storage_backend,\n    path=\".cache/experiments\",\n    cache=True,\n)\n</code></pre>"}]}