{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Themis Quick Start Tutorial\n",
    "\n",
    "This notebook demonstrates the basics of using Themis for LLM evaluation.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, make sure Themis is installed:\n",
    "\n",
    "```bash\n",
    "pip install themis-eval[math,nlp]\n",
    "# or\n",
    "uv pip install themis-eval[math,nlp]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Themis\n",
    "from themis import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Your First Evaluation\n",
    "\n",
    "Let's evaluate a model on a built-in benchmark. We'll use the `demo` benchmark which has only 10 samples - perfect for testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on demo benchmark (10 samples)\n",
    "result = evaluate(\n",
    "    benchmark=\"demo\",\n",
    "    model=\"fake-math-llm\",  # Fake model for testing (no API key needed)\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {result.evaluation_report.metrics['ExactMatch'].mean:.2%}\")\n",
    "print(f\"Total samples: {len(result.generation_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Available Benchmarks\n",
    "\n",
    "Themis includes 6 built-in benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from themis.presets import list_benchmarks\n",
    "\n",
    "benchmarks = list_benchmarks()\n",
    "print(\"Available benchmarks:\")\n",
    "for benchmark in benchmarks:\n",
    "    print(f\"  - {benchmark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Real Models\n",
    "\n",
    "To use real models, set your API keys as environment variables:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
    "```\n",
    "\n",
    "Then evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run with your API key\n",
    "# result = evaluate(\n",
    "#     benchmark=\"gsm8k\",\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     limit=10,  # Start small\n",
    "# )\n",
    "# print(f\"Accuracy: {result.evaluation_report.metrics['ExactMatch'].mean:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Configuration\n",
    "\n",
    "You can customize sampling parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    benchmark=\"demo\",\n",
    "    model=\"fake-math-llm\",\n",
    "    temperature=0.7,      # Sampling temperature\n",
    "    max_tokens=256,       # Max response length\n",
    "    num_samples=1,        # Samples per prompt\n",
    "    workers=4,            # Parallel workers\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "print(f\"Results: {result.evaluation_report.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View Detailed Results\n",
    "\n",
    "The result object contains all evaluation details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the report\n",
    "print(\"Run ID:\", result.run_id)\n",
    "print(\"Metrics:\", result.evaluation_report.metrics)\n",
    "print(\"Number of samples:\", len(result.generation_results))\n",
    "\n",
    "# View the full report\n",
    "print(\"\\nFull Report:\")\n",
    "print(result.report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Caching and Resume\n",
    "\n",
    "Themis automatically caches results. Specify a `run_id` to resume failed runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run\n",
    "result1 = evaluate(\n",
    "    benchmark=\"demo\",\n",
    "    model=\"fake-math-llm\",\n",
    "    run_id=\"my-experiment\",\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "print(\"First run completed\")\n",
    "\n",
    "# Second run with same run_id (will use cache)\n",
    "result2 = evaluate(\n",
    "    benchmark=\"demo\",\n",
    "    model=\"fake-math-llm\",\n",
    "    run_id=\"my-experiment\",\n",
    "    resume=True,  # Use cached results\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "print(\"Second run used cache (instant!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Dataset\n",
    "\n",
    "You can evaluate on your own data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom dataset\n",
    "dataset = [\n",
    "    {\"prompt\": \"What is 2+2?\", \"answer\": \"4\"},\n",
    "    {\"prompt\": \"What is 5+3?\", \"answer\": \"8\"},\n",
    "    {\"prompt\": \"What is 10-7?\", \"answer\": \"3\"},\n",
    "]\n",
    "\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    model=\"fake-math-llm\",\n",
    "    prompt=\"Question: {prompt}\\nAnswer:\",\n",
    "    metrics=[\"ExactMatch\"],\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {result.evaluation_report.metrics['ExactMatch'].mean:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Tutorial 02**: Learn how to compare multiple runs\n",
    "- **Tutorial 03**: Explore custom metrics and advanced features\n",
    "- **Documentation**: Check out [docs/index.md](../docs/index.md)\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "- âœ… How to run evaluations with `evaluate()`\n",
    "- âœ… Using built-in benchmarks\n",
    "- âœ… Customizing model parameters\n",
    "- âœ… Caching and resuming runs\n",
    "- âœ… Evaluating custom datasets\n",
    "\n",
    "Happy evaluating! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
