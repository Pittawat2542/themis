{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Themis Comparison Tutorial\n",
    "\n",
    "This notebook demonstrates how to compare multiple experiment runs using statistical tests.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from themis import evaluate\n",
    "from themis.comparison import compare_runs\n",
    "from themis.comparison.statistics import StatisticalTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Multiple Experiments\n",
    "\n",
    "First, let's run evaluations with different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Temperature = 0.0\n",
    "result1 = evaluate(\n",
    "    benchmark=\"demo\",\n",
    "    model=\"fake-math-llm\",\n",
    "    temperature=0.0,\n",
    "    run_id=\"temp-0.0\",\n",
    "    limit=10,\n",
    ")\n",
    "print(f\"Temp 0.0 accuracy: {result1.metrics['ExactMatch']:.2%}\")\n",
    "\n",
    "# Experiment 2: Temperature = 0.7\n",
    "result2 = evaluate(\n",
    "    benchmark=\"demo\",\n",
    "    model=\"fake-math-llm\",\n",
    "    temperature=0.7,\n",
    "    run_id=\"temp-0.7\",\n",
    "    limit=10,\n",
    ")\n",
    "print(f\"Temp 0.7 accuracy: {result2.metrics['ExactMatch']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Two Runs\n",
    "\n",
    "Now let's compare them statistically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with bootstrap test (default)\n",
    "report = compare_runs(\n",
    "    run_ids=[\"temp-0.0\", \"temp-0.7\"],\n",
    "    storage_path=\".cache/experiments\",\n",
    "    statistical_test=StatisticalTest.BOOTSTRAP,\n",
    "    alpha=0.05,  # 95% confidence\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Access Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall best run\n",
    "print(f\"Overall best: {report.overall_best_run}\")\n",
    "\n",
    "# Best per metric\n",
    "for metric, run_id in report.best_run_per_metric.items():\n",
    "    print(f\"{metric}: {run_id}\")\n",
    "\n",
    "# Detailed pairwise results\n",
    "for result in report.pairwise_results:\n",
    "    print(f\"\\n{result.summary()}\")\n",
    "    if result.is_significant():\n",
    "        print(\"  âœ“ Statistically significant\")\n",
    "    else:\n",
    "        print(\"  âœ— Not statistically significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Different Statistical Tests\n",
    "\n",
    "Try different statistical tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test\n",
    "report_ttest = compare_runs(\n",
    "    run_ids=[\"temp-0.0\", \"temp-0.7\"],\n",
    "    storage_path=\".cache/experiments\",\n",
    "    statistical_test=StatisticalTest.T_TEST,\n",
    ")\n",
    "\n",
    "print(\"T-test Results:\")\n",
    "for result in report_ttest.pairwise_results:\n",
    "    if result.test_result:\n",
    "        print(f\"  {result.metric_name}: p={result.test_result.p_value:.4f}\")\n",
    "        if result.test_result.effect_size:\n",
    "            print(f\"  Effect size (Cohen's d): {result.test_result.effect_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation test\n",
    "report_perm = compare_runs(\n",
    "    run_ids=[\"temp-0.0\", \"temp-0.7\"],\n",
    "    storage_path=\".cache/experiments\",\n",
    "    statistical_test=StatisticalTest.PERMUTATION,\n",
    "    alpha=0.01,  # 99% confidence\n",
    ")\n",
    "\n",
    "print(\"\\nPermutation Test Results:\")\n",
    "for result in report_perm.pairwise_results:\n",
    "    if result.test_result:\n",
    "        print(f\"  {result.metric_name}: p={result.test_result.p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Multiple Runs\n",
    "\n",
    "Compare 3+ runs with win/loss matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run third experiment\n",
    "result3 = evaluate(\n",
    "    benchmark=\"demo\",\n",
    "    model=\"fake-math-llm\",\n",
    "    temperature=1.0,\n",
    "    run_id=\"temp-1.0\",\n",
    "    limit=10,\n",
    ")\n",
    "print(f\"Temp 1.0 accuracy: {result3.metrics['ExactMatch']:.2%}\")\n",
    "\n",
    "# Compare all three\n",
    "report_multi = compare_runs(\n",
    "    run_ids=[\"temp-0.0\", \"temp-0.7\", \"temp-1.0\"],\n",
    "    storage_path=\".cache/experiments\",\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(report_multi.summary(include_details=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Win/Loss Matrix\n",
    "\n",
    "View pairwise comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get win/loss matrix for ExactMatch\n",
    "matrix = report_multi.win_loss_matrices[\"ExactMatch\"]\n",
    "\n",
    "print(\"Win/Loss Matrix for ExactMatch:\")\n",
    "print(matrix.to_table())\n",
    "\n",
    "# Rankings\n",
    "print(\"\\nRankings:\")\n",
    "for rank, (run_id, wins, losses, ties) in enumerate(matrix.rank_runs(), 1):\n",
    "    print(f\"{rank}. {run_id}: {wins}W-{losses}L-{ties}T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "\n",
    "Export comparison reports to various formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Export to JSON\n",
    "report_dict = report_multi.to_dict()\n",
    "output_file = Path(\"comparison_report.json\")\n",
    "output_file.write_text(json.dumps(report_dict, indent=2))\n",
    "print(f\"Exported to: {output_file}\")\n",
    "\n",
    "# View structure\n",
    "print(\"\\nReport keys:\", list(report_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Programmatic Statistics\n",
    "\n",
    "Use the statistics module directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from themis.comparison.statistics import t_test, bootstrap_confidence_interval\n",
    "\n",
    "# Example scores\n",
    "model_a_scores = [0.85, 0.87, 0.83, 0.90, 0.82, 0.88, 0.84, 0.86]\n",
    "model_b_scores = [0.78, 0.80, 0.79, 0.82, 0.77, 0.81, 0.80, 0.79]\n",
    "\n",
    "# T-test\n",
    "t_result = t_test(model_a_scores, model_b_scores, paired=True)\n",
    "print(\"T-test:\")\n",
    "print(f\"  Statistic: {t_result.statistic:.3f}\")\n",
    "print(f\"  P-value: {t_result.p_value:.4f}\")\n",
    "print(f\"  Significant: {t_result.significant}\")\n",
    "print(f\"  Effect size: {t_result.effect_size:.3f}\")\n",
    "\n",
    "# Bootstrap\n",
    "boot_result = bootstrap_confidence_interval(\n",
    "    model_a_scores, model_b_scores, n_bootstrap=10000, confidence_level=0.95, seed=42\n",
    ")\n",
    "print(\"\\nBootstrap:\")\n",
    "print(f\"  CI: {boot_result.confidence_interval}\")\n",
    "print(f\"  Significant: {boot_result.significant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "- âœ… How to compare two or more runs\n",
    "- âœ… Using different statistical tests (t-test, bootstrap, permutation)\n",
    "- âœ… Interpreting p-values and significance\n",
    "- âœ… Win/loss matrices for multiple runs\n",
    "- âœ… Exporting comparison results\n",
    "- âœ… Using statistics functions directly\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Learn about [custom metrics](../docs/EVALUATION.md)\n",
    "- Explore [backend extensibility](../docs/EXTENDING_BACKENDS.md)\n",
    "- Check out the [API server](../docs/API_SERVER.md)\n",
    "\n",
    "Happy comparing! ðŸ”¬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
